{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b34e3602-be30-4320-98bb-a4e67fa175e8",
   "metadata": {
    "id": "VX-6Z_MfIR1T",
    "tags": []
   },
   "source": [
    "# chiTraモデルのファインチューニング"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dfe4cb-ce55-4ebc-af43-19be0ddf0d03",
   "metadata": {
    "id": "VX-6Z_MfIR1T",
    "tags": []
   },
   "source": [
    "前のノートブックで見たように、配布されているchiTraモデルは「マスクされた語を予測する」ように学習された状態で、そのままでは他のタスクには利用できません。\n",
    "そこで、応用したいタスクのデータに対して望む出力ができるよう調整を行います。これがファインチューニングと呼ばれる作業です。\n",
    "\n",
    "このノートブックでは、\n",
    "- chiTraモデルを実際のタスクに合わせてファインチューニングし、その効果を確認すること\n",
    "- 訓練したモデルをデプロイし、実際に使ってみること\n",
    "\n",
    "がゴールです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb11ea6-cb23-4a48-aa2e-504714c449c6",
   "metadata": {
    "id": "ZDS2ZaefIb7U",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c663b59f-0de4-4f0e-961f-80a75eb61713",
   "metadata": {},
   "source": [
    "### 出力の消去\n",
    "配布のノートブックでは、各セルの実行結果を参照用に残しています。\n",
    "\n",
    "作業においては実行場所がわかりにくくなるので、右クリックのメニューから`Clear All Outputs`を実行して消去します。\n",
    "\n",
    "本来どのようになるのか確認したい、初期状態に戻したいなどの場合は、\n",
    "ターミナルからコマンド `tar -xvf notebooks.tar.gz` で再度展開を行ってください（ファイルの上書きにご注意ください）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93625c24-6f31-4ece-93b0-db12a62d3a75",
   "metadata": {},
   "source": [
    "### 定数\n",
    "chiTraモデルデータへのパスのほか、AWS内のリソースへのパスなど、\n",
    "ノートブック内で使用する定数をいくつか定義します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e427ab-6c88-408c-be0a-3218d9958049",
   "metadata": {},
   "outputs": [],
   "source": [
    "chitra_path = \"./chiTra-1.0\"\n",
    "\n",
    "s3_handson_bucket = \"chitra-handson-20221203\"\n",
    "s3_source_path = f\"s3://{s3_handson_bucket}/source/sourcedir.tar.gz\"\n",
    "s3_data_path = f\"s3://{s3_handson_bucket}/datasets/livedoor\"\n",
    "s3_output_path = f\"s3://{s3_handson_bucket}/trained\"\n",
    "\n",
    "rawdata_path = f\"{s3_data_path}/raw\"\n",
    "train_input_path = f\"{s3_data_path}/train\"\n",
    "test_input_path = f\"{s3_data_path}/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8196de0f-90d7-4ecf-9148-23d8ed60fce8",
   "metadata": {
    "id": "-23O_0VYcx1i",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## タスクデータの準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8014aca8-d684-4d8a-9c0a-5ea94d72d6ec",
   "metadata": {
    "id": "-23O_0VYcx1i",
    "tags": []
   },
   "source": [
    "今回は [livedoor ニュースコーパス](https://www.rondhuit.com/download.html#ldcc)を利用します。\n",
    "こちらは「livedoor ニュース」の９つのニュースカテゴリから記事を収集したものです。\n",
    "\n",
    "記事本文の内容を入力として、どのカテゴリの記事かを判定するタスクを対象としてモデルをチューニングしていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a109ec6-f655-45b8-a829-4792ef95335e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### データの確認"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f34cc84-5d64-4ab9-a723-543b86cbf03b",
   "metadata": {
    "id": "-23O_0VYcx1i",
    "tags": []
   },
   "source": [
    "まずはデータセットの内容を少し見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7c5eac0-8924-4d7e-ab4e-451c2cfd1eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 からデータをダウンロード\n",
    "import datasets\n",
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()\n",
    "livedoor_data = datasets.load_from_disk(rawdata_path, fs=s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c2223da-7690-4d09-b06b-a98834506ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence1', 'label'],\n",
      "    num_rows: 7367\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# データの量を確認\n",
    "print(livedoor_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95370667-a499-42c8-aeac-ef6eb2cdeb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ラベル一覧 ['dokujo-tsushin', 'it-life-hack', 'kaden-channel', 'livedoor-homme', 'movie-enter', 'peachy', 'smax', 'sports-watch', 'topic-news']\n"
     ]
    }
   ],
   "source": [
    "# カテゴリラベルの一覧\n",
    "labels = list(sorted(set(livedoor_data[\"label\"])))\n",
    "print(\"ラベル一覧\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f69e145-5c4f-4799-b00f-8837ebf7b1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ラベル: it-life-hack\n",
      "本文: 戻ってまいりました！先日あえなく砕け散ったインテルの最新SSD「520シリーズ」を旧型Macに取りつけるという連載。今回から装いも新たに、かつ名誉挽回を目指し前回とは異なるアプローチで装着させることを試みた。旧MacがSSDによる「快適動作」に成功するかどうか、乞うご期待。 ■SSD外付けでの起動にアプローチ 以前の「インテル SSD 520を旧Macに装着」シリーズは、結局、MacProでの動作に成功しないまま終了してしまった。筆者はSATAコントローラに問題があると推測したが、発売後５年以上を経た今、コントローラの改善は期待できない。そこで、SSDを既存のHDDスペースに装着する以外の方法を試してみることにした。 Macは、Windowsと異なり、外付け機器からの起動を広くサポートしている。標準で、USB接続の外付けHDDやフラッシュメモリ、FireWire（IEEE1394）接続の外...\n"
     ]
    }
   ],
   "source": [
    "# データをランダムに表示\n",
    "data = livedoor_data.shuffle()[0]\n",
    "\n",
    "print(\"ラベル:\", data['label'])\n",
    "print(\"本文:\", f\"{data['sentence1'][:400]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4b2d8c-5d04-405f-9db4-e317e5970665",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### データの加工"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86c06a9-5f41-4e78-adb6-a61edb2c81b5",
   "metadata": {},
   "source": [
    "前のノートブックで見たように、chiTraモデルはテキストをそのまま扱うことはできないので、chiTraトークナイザを使って加工を行います。\n",
    "\n",
    "また併せてカテゴリラベルについても数値へ変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95e82d7f-c0d8-42c3-8a82-6b7b0f8cc288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee09612d796c450587bc9b702d376545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sudachitra as chitra\n",
    "\n",
    "# トークナイザを読み込む\n",
    "tok = chitra.BertSudachipyTokenizer.from_pretrained(chitra_path)\n",
    "\n",
    "# ラベルを数値に変換・逆変換する\n",
    "label2id = {l:i for i,l in enumerate(labels)}\n",
    "id2label = {i:l for i,l in enumerate(labels)}\n",
    "\n",
    "# データをモデル入力用に整形する関数\n",
    "def preprocess(data):\n",
    "    # chiTraトークナイザを適用\n",
    "    ret = tok(data[\"sentence1\"], padding=True, truncation=True, max_length=512)\n",
    "    # ラベルを変換\n",
    "    ret[\"label\"] = [label2id[l] for l in data[\"label\"]]\n",
    "    return ret\n",
    "\n",
    "# 加工の実行（ここでは一部のみ）\n",
    "processed_data = livedoor_data.shuffle().select(range(100)).map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=[\"sentence1\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a144f74b-df5c-4cb6-bb87-36791f989655",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 1, 'input_ids': [2, 19559, 18763, 484, 10309, 1432, 16694, 2476, 485, 418, 16295, 519, 11254, 476, 14922, 480, 12116, 519, 461, 476, 10146, 419, 12861, 1083, 6514, 484, 11316, 6139, 6740, 481, 10653, 11632, 450, 11098, 477, 12903, 478, 10152, 476, 10146, 419, 10209, 485, 418, 13962, 484, 1106, 5388, 10148, 11499, 469, 419, 1432, 11476, 485, 418, 427, 23905, 11477, 23488, 22031, 484, 10992, 11791, 477, 418, 19559, 484, 11316, 6139, 6740, 484, 12370, 12244, 459, 514, 469, 12610, 450, 11187, 459, 514, 476, 442, 11439, 419, 27044, 12135, 11370, 450, 15154, 481, 12244, 461, 11439, 419, 63, 30620, 5305, 6507, 7312, 7312, 69, 5346, 5298, 5305, 16, 24995, 17, 63, 5300, 5537, 20, 29118, 428, 478, 418, 28064, 5810, 5497, 469, 419, 10154, 1106, 5388, 481, 10272, 476, 418, 20734, 484, 13307, 450, 14098, 10158, 476, 10146, 419, 427, 11847, 461, 476, 442, 11439, 428, 427, 693, 481, 616, 5649, 15574, 10811, 10152, 476, 504, 15621, 485, 1214, 481, 13242, 476, 781, 10400, 2052, 6852, 461, 476, 10715, 10148, 419, 2120, 10148, 11744, 15738, 15362, 419, 12914, 450, 10568, 11263, 477, 10874, 519, 19077, 481, 1, 428, 427, 10453, 481, 10699, 476, 16383, 484, 11553, 11791, 10336, 418, 23565, 3025, 480, 11791, 477, 4774, 5390, 480, 510, 51, 428, 427, 13962, 15457, 10389, 10145, 657, 504, 18799, 20890, 477, 12244, 10145, 11939, 461, 483, 1, 428, 427, 20522, 10148, 20740, 476, 22017, 469, 478, 23362, 5523, 16224, 450, 19416, 909, 476, 11295, 11939, 480, 419, 428, 427, 1717, 485, 418, 10983, 5423, 484, 11665, 6858, 450, 10665, 419, 428, 427, 1414, 10257, 448, 5590, 24468, 11, 1, 403, 1, 12, 484, 428, 427, 16373, 450, 19422, 10181, 469, 10151, 484, 3950, 11555, 477, 485, 1, 428, 427, 31252, 519, 17905, 481, 419, 428, 427, 20522, 1778, 461, 476, 10160, 428, 10604, 480, 11752, 450, 14098, 10158, 476, 10146, 419, 399, 1432, 16694, 11, 22996, 10810, 10359, 12, 11721, 16295, 399, 1432, 11476, 481, 11043, 461, 469, 11791, 519, 15618, 11654, 610, 13563, 484, 16245, 519, 11822, 11439, 6, 1432, 11476, 4981, 14156, 15282, 484, 1717, 481, 504, 2123, 485, 12431, 13888, 4981, 610, 16295, 484, 11086, 484, 2091, 450, 535, 14006, 5497, 5497, 6, 1432, 11476, 26102, 10323, 11632, 450, 12903, 610, 18594, 450, 477, 19676, 6, 1432, 11476, 11520, 12546, 10833, 481, 15069, 484, 1311, 610, 14108, 5950, 484, 15574, 10936, 504, 10811, 10176, 449, 480, 1, 1432, 11476, 2281, 6813, 3025, 480, 11039, 450, 12903, 610, 21404, 477, 485, 11786, 6, 1432, 11476, 16295, 14815, 484, 12116, 481, 18048, 1432, 484, 20, 645, 484, 17170, 1432, 16694, 484, 10570, 12153, 11, 19031, 5717, 13842, 12, 17028, 28, 30296, 3683, 5809, 19031, 5717, 11514, 11, 10463, 15, 18857, 15, 18061, 12, 10637, 5489, 28, 20041, 16, 18552, 5415, 5535, 5305, 1062, 24860, 519, 11654, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# 加工済みデータをランダムに表示\n",
    "print(processed_data.shuffle()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace10b78-784b-457c-adc4-4bca513dc0ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### データのアップロード"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f76cf8-3e70-486a-8640-91da1b3e55a3",
   "metadata": {},
   "source": [
    "今回は準備済みなので必要ありませんが、一から準備を行う場合はさらに、\n",
    "訓練用と確認用のデータを分割し、s3へとアップロードしておくことになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c753dc83-7174-41a4-ba4b-ad3d58a1e77e",
   "metadata": {
    "id": "c753dc83-7174-41a4-ba4b-ad3d58a1e77e",
    "outputId": "62335baa-120b-4aa0-a766-eb579b25e791"
   },
   "outputs": [],
   "source": [
    "# import botocore\n",
    "# from datasets.filesystems import S3FileSystem\n",
    "\n",
    "# split = processed_data.train_test_split(test_size=0.2)\n",
    "\n",
    "# s3 = S3FileSystem()\n",
    "# split[\"train\"].save_to_disk(f\"{train_input_path}_tmp\", fs=s3)\n",
    "# split[\"test\"].save_to_disk(f\"{test_input_path}_tmp\", fs=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145ddff7-77c1-4aec-9509-c5a6486fa1e7",
   "metadata": {
    "id": "897b819e-56bd-4098-aaf3-24bc20966a8c",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## ファインチューニング"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4531e51-2f52-40ea-b818-039565c9b2b3",
   "metadata": {
    "id": "897b819e-56bd-4098-aaf3-24bc20966a8c",
    "tags": []
   },
   "source": [
    "次に加工したデータを使ってモデルのファインチューニングを行うジョブを実行していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc186af-c6c4-4846-9652-d4bfcfa37583",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### SageMakerで訓練を行う"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313a15f1-1da9-49f6-be43-6479103184f7",
   "metadata": {
    "id": "897b819e-56bd-4098-aaf3-24bc20966a8c",
    "tags": []
   },
   "source": [
    "SageMaker SDKライブラリを用いることで、ノートブックから訓練ジョブを作成することができます。\n",
    "\n",
    "APIを叩く形になるためここでのコードはシンプルで、事前に用意したリソースや訓練の細かい設定を引数として渡していくことになります。\n",
    "\n",
    "学習には少々時間がかかるので、まずはジョブを実行してしまいましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "927ab306-b868-43aa-995a-7229a0364b50",
   "metadata": {
    "id": "b3c487ce-b78d-47a3-a2af-2fa779f2a773"
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# SageMakerへのセッションを確立\n",
    "sess = sagemaker.Session(default_bucket=s3_handson_bucket)\n",
    "\n",
    "# 訓練ジョブの定義\n",
    "huggingface_estimator = HuggingFace(\n",
    "    sagemaker_session=sess,          # セッション\n",
    "    output_path=s3_output_path,      # 訓練結果の出力先\n",
    "    source_dir=s3_source_path,       # 訓練用リソースの置き場所（後で説明）\n",
    "    entry_point='train.py',          # 訓練スクリプトの名称\n",
    "    instance_type='ml.g4dn.xlarge',  # 学習に使用するインスタンス\n",
    "    instance_count=1,\n",
    "    role=\"arn:aws:iam::012345678901:role/ChitraHandsonSageMakerExecution\",\n",
    "    transformers_version='4.12',\n",
    "    pytorch_version='1.9',\n",
    "    py_version='py38',\n",
    "    hyperparameters = {\n",
    "        'num_labels': len(labels), # 今回のタスクのラベル数\n",
    "        'epochs': 1,               # チューニングの量\n",
    "        'train-batch-size': 12,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "834f6868-f4a5-4c12-b331-fb86fd7ca1b9",
   "metadata": {
    "id": "834f6868-f4a5-4c12-b331-fb86fd7ca1b9",
    "outputId": "d59a935c-734d-4962-f484-ad3f223e80f1",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-30 10:05:46 Starting - Starting the training job...\n",
      "2022-11-30 10:06:09 Starting - Preparing the instances for trainingProfilerReport-1669802745: InProgress\n",
      "............\n",
      "2022-11-30 10:08:18 Downloading - Downloading input data\n",
      "2022-11-30 10:08:18 Training - Downloading the training image............\n",
      "2022-11-30 10:10:30 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-11-30 10:10:36,000 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-11-30 10:10:36,026 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-11-30 10:10:36,029 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-11-30 10:10:40,836 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting sudachitra\u001b[0m\n",
      "\u001b[34mDownloading SudachiTra-0.1.7.tar.gz (283 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 283.7/283.7 kB 26.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting datasets>=2.6\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.7.1-py3-none-any.whl (451 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 451.7/451.7 kB 62.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting logzero~=1.7.0\u001b[0m\n",
      "\u001b[34mDownloading logzero-1.7.0-py2.py3-none-any.whl (16 kB)\u001b[0m\n",
      "\u001b[34mCollecting progressbar2~=3.53.1\u001b[0m\n",
      "\u001b[34mDownloading progressbar2-3.53.3-py2.py3-none-any.whl (25 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers>=0.10.3 in /opt/conda/lib/python3.8/site-packages (from sudachitra->-r requirements.txt (line 1)) (0.10.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers>=4.6.1 in /opt/conda/lib/python3.8/site-packages (from sudachitra->-r requirements.txt (line 1)) (4.12.3)\u001b[0m\n",
      "\u001b[34mCollecting sudachipy>=0.6.2\u001b[0m\n",
      "\u001b[34mDownloading SudachiPy-0.6.6-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 106.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting sudachidict_core>=20210802\u001b[0m\n",
      "\u001b[34mDownloading SudachiDict-core-20221021.tar.gz (9.0 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7 in /opt/conda/lib/python3.8/site-packages (from datasets>=2.6->-r requirements.txt (line 4)) (0.3.5.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets>=2.6->-r requirements.txt (line 4)) (3.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets>=2.6->-r requirements.txt (line 4)) (1.22.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.8/site-packages (from datasets>=2.6->-r requirements.txt (line 4)) (2022.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=2.6->-r requirements.txt (line 4)) (2.27.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets>=2.6->-r requirements.txt (line 4)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=2.6->-r requirements.txt (line 4)) (6.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from datasets>=2.6->-r requirements.txt (line 4)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets>=2.6->-r requirements.txt (line 4)) (1.2.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets>=2.6->-r requirements.txt (line 4)) (4.64.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets>=2.6->-r requirements.txt (line 4)) (0.70.13)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets>=2.6->-r requirements.txt (line 4)) (21.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=2.6->-r requirements.txt (line 4)) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets>=2.6->-r requirements.txt (line 4)) (3.10.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets>=2.6->-r requirements.txt (line 4)) (3.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing<3,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets>=2.6->-r requirements.txt (line 4)) (2.4.7)\u001b[0m\n",
      "\u001b[34mCollecting python-utils>=2.3.0\u001b[0m\n",
      "\u001b[34mDownloading python_utils-3.4.5-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from progressbar2~=3.53.1->sudachitra->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=2.6->-r requirements.txt (line 4)) (2022.5.18.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=2.6->-r requirements.txt (line 4)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=2.6->-r requirements.txt (line 4)) (2.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=2.6->-r requirements.txt (line 4)) (1.26.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers>=4.6.1->sudachitra->-r requirements.txt (line 1)) (2022.6.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers>=4.6.1->sudachitra->-r requirements.txt (line 1)) (0.0.53)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=2.6->-r requirements.txt (line 4)) (21.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=2.6->-r requirements.txt (line 4)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=2.6->-r requirements.txt (line 4)) (6.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=2.6->-r requirements.txt (line 4)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=2.6->-r requirements.txt (line 4)) (1.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=2.6->-r requirements.txt (line 4)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets>=2.6->-r requirements.txt (line 4)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets>=2.6->-r requirements.txt (line 4)) (2021.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers>=4.6.1->sudachitra->-r requirements.txt (line 1)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers>=4.6.1->sudachitra->-r requirements.txt (line 1)) (8.1.3)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sudachitra, sudachidict_core\u001b[0m\n",
      "\u001b[34mBuilding wheel for sudachitra (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for sudachitra (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for sudachitra: filename=SudachiTra-0.1.7-py3-none-any.whl size=266086 sha256=ddbca621a0cb32b5660277d831e0b82743b775eb8cbfd9ce5cc02388ce49775d\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/97/ad/02/69094014ac4631f81c227d7cf059fd46c11df9e5bfd03c6f41\u001b[0m\n",
      "\u001b[34mBuilding wheel for sudachidict_core (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for sudachidict_core (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for sudachidict_core: filename=SudachiDict_core-20221021-py3-none-any.whl size=71574762 sha256=a92747767647c7b2b0d89e8cf638539fc26ec8c5fd83c60e02b245bf1074f3a5\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/ca/e3/ed/e78fecf6fd34349114d292242a16fc08d513fb32c2d9c5d786\u001b[0m\n",
      "\u001b[34mSuccessfully built sudachitra sudachidict_core\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sudachipy, logzero, sudachidict_core, python-utils, responses, progressbar2, sudachitra, datasets\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 1.15.1\u001b[0m\n",
      "\u001b[34mUninstalling datasets-1.15.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-1.15.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed datasets-2.7.1 logzero-1.7.0 progressbar2-3.53.3 python-utils-3.4.5 responses-0.18.0 sudachidict_core-20221021 sudachipy-0.6.6 sudachitra-0.1.7\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.1.2 -> 22.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2022-11-30 10:11:03,037 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2022-11-30 10:11:03,037 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2022-11-30 10:11:03,118 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1,\n",
      "        \"num_labels\": 9,\n",
      "        \"train-batch-size\": 12\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2022-11-30-10-05-45-257\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://chitra-handson-20221203/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"num_labels\":9,\"train-batch-size\":12}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://chitra-handson-20221203/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"num_labels\":9,\"train-batch-size\":12},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2022-11-30-10-05-45-257\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://chitra-handson-20221203/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--num_labels\",\"9\",\"--train-batch-size\",\"12\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_LABELS=9\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN-BATCH-SIZE=12\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python train.py --epochs 1 --num_labels 9 --train-batch-size 12\u001b[0m\n",
      "\u001b[34m2022-11-30 10:11:04,865 - __main__ - INFO -  loaded train_dataset length is: 5894\u001b[0m\n",
      "\u001b[34m2022-11-30 10:11:04,865 - __main__ - INFO -  loaded test_dataset length is: 737\u001b[0m\n",
      "\u001b[34mtrain.py:54: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 4.21kB [00:00, 4.86MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at ./chiTra-1.0 were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of BertForSequenceClassification were not initialized from the model checkpoint at ./chiTra-1.0 and are newly initialized: ['classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at ./chiTra-1.0 were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of BertForSequenceClassification were not initialized from the model checkpoint at ./chiTra-1.0 and are newly initialized: ['classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m***** Running training *****\n",
      "  Num examples = 5894\n",
      "  Num Epochs = 1\u001b[0m\n",
      "\u001b[34mInstantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m***** Running training *****\n",
      "  Num examples = 5894\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34mTotal optimization steps = 492\u001b[0m\n",
      "\u001b[34mTotal optimization steps = 492\u001b[0m\n",
      "\u001b[34m0%|          | 0/492 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.474 algo-1:66 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.503 algo-1:66 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.504 algo-1:66 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.504 algo-1:66 INFO hook.py:200] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.505 algo-1:66 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.505 algo-1:66 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.591 algo-1:66 INFO hook.py:591] name:bert.embeddings.word_embeddings.weight count_params:25048320\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.591 algo-1:66 INFO hook.py:591] name:bert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.591 algo-1:66 INFO hook.py:591] name:bert.embeddings.token_type_embeddings.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.591 algo-1:66 INFO hook.py:591] name:bert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.591 algo-1:66 INFO hook.py:591] name:bert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.591 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.591 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.591 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.591 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.591 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.591 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.591 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.0.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.0.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.0.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.0.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.0.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.0.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.0.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.0.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.1.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.1.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.1.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.1.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.1.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.1.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.1.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.1.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.2.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.2.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.2.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.2.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.2.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.2.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.2.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.2.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.3.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.3.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.3.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.3.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.3.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.3.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.3.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.3.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.4.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.4.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.4.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.4.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.4.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.4.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.4.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.4.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.5.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.5.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.5.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.5.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.5.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.5.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.5.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.5.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.6.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.6.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.6.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.6.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.6.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.6.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.6.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.6.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.7.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.7.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.7.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.7.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.7.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.7.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.7.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.7.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.8.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.8.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.8.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.8.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.8.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.8.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.8.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.8.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.9.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.9.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.9.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.9.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.9.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.9.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.9.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.9.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.10.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.10.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.10.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.10.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.10.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.10.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.10.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.10.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.11.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.11.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.600 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.600 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.600 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.11.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.600 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.11.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.600 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.11.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.600 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.11.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.600 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.11.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.600 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.11.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.600 algo-1:66 INFO hook.py:591] name:bert.pooler.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.600 algo-1:66 INFO hook.py:591] name:bert.pooler.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.600 algo-1:66 INFO hook.py:591] name:classifier.weight count_params:6912\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.600 algo-1:66 INFO hook.py:591] name:classifier.bias count_params:9\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.600 algo-1:66 INFO hook.py:593] Total Trainable Params: 111096585\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.600 algo-1:66 INFO hook.py:424] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.601 algo-1:66 INFO hook.py:488] Hook is writing from the hook with pid: 66\u001b[0m\n",
      "\u001b[34m0%|          | 1/492 [00:02<21:19,  2.61s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 2/492 [00:03<13:45,  1.68s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 3/492 [00:04<11:18,  1.39s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 4/492 [00:05<10:09,  1.25s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 5/492 [00:06<09:30,  1.17s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 6/492 [00:07<09:07,  1.13s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 7/492 [00:08<08:52,  1.10s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 8/492 [00:09<08:43,  1.08s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 9/492 [00:10<08:36,  1.07s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 10/492 [00:11<08:30,  1.06s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 11/492 [00:12<08:26,  1.05s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 12/492 [00:14<08:24,  1.05s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 13/492 [00:15<08:20,  1.05s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 14/492 [00:16<08:20,  1.05s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 15/492 [00:17<08:21,  1.05s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 16/492 [00:18<08:18,  1.05s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 17/492 [00:19<08:17,  1.05s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 18/492 [00:20<08:17,  1.05s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 19/492 [00:21<08:16,  1.05s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 20/492 [00:22<08:15,  1.05s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 21/492 [00:23<08:12,  1.04s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 22/492 [00:24<08:12,  1.05s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 23/492 [00:25<08:13,  1.05s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 24/492 [00:26<08:12,  1.05s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 25/492 [00:27<08:12,  1.05s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 26/492 [00:28<08:09,  1.05s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 27/492 [00:29<08:08,  1.05s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 28/492 [00:30<08:07,  1.05s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 29/492 [00:31<08:07,  1.05s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 30/492 [00:32<08:05,  1.05s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 31/492 [00:33<08:05,  1.05s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 32/492 [00:35<08:05,  1.05s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 33/492 [00:36<08:04,  1.06s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 34/492 [00:37<08:03,  1.06s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 35/492 [00:38<08:03,  1.06s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 36/492 [00:39<08:03,  1.06s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 37/492 [00:40<08:04,  1.06s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 38/492 [00:41<08:02,  1.06s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 39/492 [00:42<08:03,  1.07s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 40/492 [00:43<08:03,  1.07s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 41/492 [00:44<08:03,  1.07s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 42/492 [00:45<08:02,  1.07s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 43/492 [00:46<08:05,  1.08s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 44/492 [00:47<08:02,  1.08s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 45/492 [00:48<07:59,  1.07s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 46/492 [00:50<07:57,  1.07s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 47/492 [00:51<07:58,  1.08s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 48/492 [00:52<07:56,  1.07s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 49/492 [00:53<07:55,  1.07s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 50/492 [00:54<07:55,  1.08s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 51/492 [00:55<07:54,  1.08s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 52/492 [00:56<07:54,  1.08s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 53/492 [00:57<07:54,  1.08s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 54/492 [00:58<07:54,  1.08s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 55/492 [00:59<07:54,  1.09s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 56/492 [01:00<07:50,  1.08s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 57/492 [01:01<07:50,  1.08s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 58/492 [01:02<07:48,  1.08s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 59/492 [01:04<07:47,  1.08s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 60/492 [01:05<07:48,  1.08s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 61/492 [01:06<07:47,  1.08s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 62/492 [01:07<07:47,  1.09s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 63/492 [01:08<07:46,  1.09s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 64/492 [01:09<07:44,  1.09s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 65/492 [01:10<07:43,  1.09s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 66/492 [01:11<07:41,  1.08s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 67/492 [01:12<07:38,  1.08s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 68/492 [01:13<07:38,  1.08s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 69/492 [01:14<07:37,  1.08s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 70/492 [01:16<07:39,  1.09s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 71/492 [01:17<07:37,  1.09s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 72/492 [01:18<07:37,  1.09s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 73/492 [01:19<07:36,  1.09s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 74/492 [01:20<07:35,  1.09s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 75/492 [01:21<07:34,  1.09s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 76/492 [01:22<07:33,  1.09s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 77/492 [01:23<07:33,  1.09s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 78/492 [01:24<07:33,  1.10s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 79/492 [01:25<07:33,  1.10s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 80/492 [01:26<07:32,  1.10s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 81/492 [01:28<07:30,  1.10s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 82/492 [01:29<07:29,  1.10s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 83/492 [01:30<07:27,  1.09s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 84/492 [01:31<07:26,  1.10s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 85/492 [01:32<07:26,  1.10s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 86/492 [01:33<07:25,  1.10s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 87/492 [01:34<07:25,  1.10s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 88/492 [01:35<07:25,  1.10s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 89/492 [01:36<07:24,  1.10s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 90/492 [01:37<07:23,  1.10s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 91/492 [01:39<07:22,  1.10s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 92/492 [01:40<07:23,  1.11s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 93/492 [01:41<07:22,  1.11s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 94/492 [01:42<07:21,  1.11s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 95/492 [01:43<07:18,  1.11s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 96/492 [01:44<07:16,  1.10s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 97/492 [01:45<07:14,  1.10s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 98/492 [01:46<07:13,  1.10s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 99/492 [01:47<07:11,  1.10s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 100/492 [01:48<07:10,  1.10s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 101/492 [01:50<07:09,  1.10s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 102/492 [01:51<07:07,  1.10s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 103/492 [01:52<07:06,  1.10s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 104/492 [01:53<07:04,  1.09s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 105/492 [01:54<07:04,  1.10s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 106/492 [01:55<07:03,  1.10s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 107/492 [01:56<07:04,  1.10s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 108/492 [01:57<07:03,  1.10s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 109/492 [01:58<07:01,  1.10s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 110/492 [01:59<07:01,  1.10s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 111/492 [02:01<07:00,  1.10s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 112/492 [02:02<06:59,  1.10s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 113/492 [02:03<06:59,  1.11s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 114/492 [02:04<06:57,  1.11s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 115/492 [02:05<06:56,  1.10s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 116/492 [02:06<06:55,  1.10s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 117/492 [02:07<06:54,  1.10s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 118/492 [02:08<06:52,  1.10s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 119/492 [02:09<06:53,  1.11s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 120/492 [02:11<06:51,  1.11s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 121/492 [02:12<06:50,  1.11s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 122/492 [02:13<06:49,  1.11s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 123/492 [02:14<06:47,  1.11s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 124/492 [02:15<06:47,  1.11s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 125/492 [02:16<06:47,  1.11s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 126/492 [02:17<06:45,  1.11s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 127/492 [02:18<06:44,  1.11s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 128/492 [02:19<06:44,  1.11s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 129/492 [02:21<06:44,  1.12s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 130/492 [02:22<06:44,  1.12s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 131/492 [02:23<06:43,  1.12s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 132/492 [02:24<06:43,  1.12s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 133/492 [02:25<06:42,  1.12s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 134/492 [02:26<06:41,  1.12s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 135/492 [02:27<06:40,  1.12s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 136/492 [02:28<06:40,  1.13s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 137/492 [02:30<06:38,  1.12s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 138/492 [02:31<06:36,  1.12s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 139/492 [02:32<06:35,  1.12s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 140/492 [02:33<06:35,  1.12s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 141/492 [02:34<06:35,  1.13s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 142/492 [02:35<06:32,  1.12s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 143/492 [02:36<06:31,  1.12s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 144/492 [02:37<06:30,  1.12s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 145/492 [02:38<06:29,  1.12s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 146/492 [02:40<06:29,  1.12s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 147/492 [02:41<06:28,  1.13s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 148/492 [02:42<06:26,  1.12s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 149/492 [02:43<06:24,  1.12s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 150/492 [02:44<06:24,  1.12s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 151/492 [02:45<06:24,  1.13s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 152/492 [02:46<06:23,  1.13s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 153/492 [02:48<06:22,  1.13s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 154/492 [02:49<06:21,  1.13s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 155/492 [02:50<06:21,  1.13s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 156/492 [02:51<06:20,  1.13s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 157/492 [02:52<06:19,  1.13s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 158/492 [02:53<06:18,  1.13s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 159/492 [02:54<06:19,  1.14s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 160/492 [02:55<06:18,  1.14s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 161/492 [02:57<06:17,  1.14s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 162/492 [02:58<06:15,  1.14s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 163/492 [02:59<06:15,  1.14s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 164/492 [03:00<06:14,  1.14s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 165/492 [03:01<06:13,  1.14s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 166/492 [03:02<06:12,  1.14s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 167/492 [03:03<06:10,  1.14s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 168/492 [03:05<06:09,  1.14s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 169/492 [03:06<06:10,  1.15s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 170/492 [03:07<06:09,  1.15s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 171/492 [03:08<06:07,  1.14s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 172/492 [03:09<06:07,  1.15s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 173/492 [03:10<06:05,  1.15s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 174/492 [03:11<06:04,  1.15s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 175/492 [03:13<06:02,  1.14s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 176/492 [03:14<06:01,  1.14s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 177/492 [03:15<06:02,  1.15s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 178/492 [03:16<06:00,  1.15s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 179/492 [03:17<05:59,  1.15s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 180/492 [03:18<05:58,  1.15s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 181/492 [03:20<05:57,  1.15s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 182/492 [03:21<05:55,  1.15s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 183/492 [03:22<05:54,  1.15s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 184/492 [03:23<05:54,  1.15s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 185/492 [03:24<05:53,  1.15s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 186/492 [03:25<05:51,  1.15s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 187/492 [03:26<05:51,  1.15s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 188/492 [03:28<05:50,  1.15s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 189/492 [03:29<05:49,  1.15s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 190/492 [03:30<05:47,  1.15s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 191/492 [03:31<05:47,  1.15s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 192/492 [03:32<05:46,  1.15s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 193/492 [03:33<05:44,  1.15s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 194/492 [03:35<05:44,  1.16s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 195/492 [03:36<05:42,  1.15s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 196/492 [03:37<05:42,  1.16s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 197/492 [03:38<05:41,  1.16s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 198/492 [03:39<05:40,  1.16s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 199/492 [03:40<05:40,  1.16s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 200/492 [03:41<05:39,  1.16s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 201/492 [03:43<05:37,  1.16s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 202/492 [03:44<05:36,  1.16s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 203/492 [03:45<05:35,  1.16s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 204/492 [03:46<05:35,  1.17s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 205/492 [03:47<05:33,  1.16s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 206/492 [03:48<05:31,  1.16s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 207/492 [03:50<05:31,  1.16s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 208/492 [03:51<05:30,  1.16s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 209/492 [03:52<05:29,  1.16s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 210/492 [03:53<05:28,  1.17s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 211/492 [03:54<05:26,  1.16s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 212/492 [03:55<05:26,  1.17s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 213/492 [03:57<05:24,  1.16s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 214/492 [03:58<05:24,  1.17s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 215/492 [03:59<05:22,  1.16s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 216/492 [04:00<05:21,  1.16s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 217/492 [04:01<05:21,  1.17s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 218/492 [04:02<05:21,  1.17s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 219/492 [04:04<05:20,  1.17s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 220/492 [04:05<05:19,  1.17s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 221/492 [04:06<05:17,  1.17s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 222/492 [04:07<05:16,  1.17s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 223/492 [04:08<05:14,  1.17s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 224/492 [04:09<05:11,  1.16s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 225/492 [04:11<05:08,  1.16s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 226/492 [04:12<05:06,  1.15s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 227/492 [04:13<05:06,  1.15s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 228/492 [04:14<05:04,  1.15s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 229/492 [04:15<05:03,  1.15s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 230/492 [04:16<05:02,  1.15s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 231/492 [04:18<05:00,  1.15s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 232/492 [04:19<05:00,  1.16s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 233/492 [04:20<04:58,  1.15s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 234/492 [04:21<04:57,  1.15s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 235/492 [04:22<04:56,  1.16s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 236/492 [04:23<04:55,  1.16s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 237/492 [04:24<04:54,  1.16s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 238/492 [04:26<04:54,  1.16s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 239/492 [04:27<04:52,  1.16s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 240/492 [04:28<04:51,  1.15s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 241/492 [04:29<04:50,  1.16s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 242/492 [04:30<04:48,  1.15s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 243/492 [04:31<04:47,  1.15s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 244/492 [04:33<04:46,  1.16s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 245/492 [04:34<04:45,  1.16s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 246/492 [04:35<04:44,  1.16s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 247/492 [04:36<04:43,  1.16s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 248/492 [04:37<04:43,  1.16s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 249/492 [04:38<04:42,  1.16s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 250/492 [04:40<04:41,  1.16s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 251/492 [04:41<04:39,  1.16s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 252/492 [04:42<04:37,  1.16s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 253/492 [04:43<04:36,  1.16s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 254/492 [04:44<04:35,  1.16s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 255/492 [04:45<04:34,  1.16s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 256/492 [04:46<04:32,  1.16s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 257/492 [04:48<04:31,  1.15s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 258/492 [04:49<04:28,  1.15s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 259/492 [04:50<04:28,  1.15s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 260/492 [04:51<04:27,  1.15s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 261/492 [04:52<04:27,  1.16s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 262/492 [04:53<04:26,  1.16s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 263/492 [04:55<04:25,  1.16s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 264/492 [04:56<04:24,  1.16s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 265/492 [04:57<04:23,  1.16s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 266/492 [04:58<04:22,  1.16s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 267/492 [04:59<04:22,  1.16s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 268/492 [05:00<04:21,  1.17s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 269/492 [05:02<04:19,  1.17s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 270/492 [05:03<04:18,  1.17s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 271/492 [05:04<04:17,  1.16s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 272/492 [05:05<04:15,  1.16s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 273/492 [05:06<04:14,  1.16s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 274/492 [05:07<04:14,  1.17s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 275/492 [05:09<04:13,  1.17s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 276/492 [05:10<04:12,  1.17s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 277/492 [05:11<04:12,  1.17s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 278/492 [05:12<04:10,  1.17s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 279/492 [05:13<04:08,  1.17s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 280/492 [05:14<04:07,  1.17s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 281/492 [05:16<04:05,  1.16s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 282/492 [05:17<04:04,  1.17s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 283/492 [05:18<04:03,  1.17s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 284/492 [05:19<04:02,  1.17s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 285/492 [05:20<04:01,  1.17s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 286/492 [05:21<04:00,  1.17s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 287/492 [05:23<03:59,  1.17s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 288/492 [05:24<03:57,  1.17s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 289/492 [05:25<03:57,  1.17s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 290/492 [05:26<03:55,  1.17s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 291/492 [05:27<03:53,  1.16s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 292/492 [05:28<03:52,  1.16s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 293/492 [05:30<03:51,  1.16s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 294/492 [05:31<03:50,  1.16s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 295/492 [05:32<03:49,  1.17s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 296/492 [05:33<03:48,  1.17s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 297/492 [05:34<03:47,  1.17s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 298/492 [05:35<03:45,  1.16s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 299/492 [05:37<03:45,  1.17s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 300/492 [05:38<03:44,  1.17s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 301/492 [05:39<03:42,  1.17s/it]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 302/492 [05:40<03:41,  1.17s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 303/492 [05:41<03:40,  1.17s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 304/492 [05:42<03:38,  1.16s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 305/492 [05:44<03:38,  1.17s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 306/492 [05:45<03:37,  1.17s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 307/492 [05:46<03:37,  1.17s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 308/492 [05:47<03:36,  1.17s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 309/492 [05:48<03:34,  1.17s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 310/492 [05:49<03:33,  1.18s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 311/492 [05:51<03:32,  1.17s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 312/492 [05:52<03:30,  1.17s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 313/492 [05:53<03:29,  1.17s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 314/492 [05:54<03:28,  1.17s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 315/492 [05:55<03:27,  1.17s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 316/492 [05:56<03:25,  1.17s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 317/492 [05:58<03:24,  1.17s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 318/492 [05:59<03:24,  1.17s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 319/492 [06:00<03:22,  1.17s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 320/492 [06:01<03:22,  1.17s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 321/492 [06:02<03:20,  1.17s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 322/492 [06:03<03:18,  1.17s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 323/492 [06:05<03:17,  1.17s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 324/492 [06:06<03:16,  1.17s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 325/492 [06:07<03:16,  1.17s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 326/492 [06:08<03:14,  1.17s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 327/492 [06:09<03:14,  1.18s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 328/492 [06:11<03:12,  1.18s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 329/492 [06:12<03:11,  1.18s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 330/492 [06:13<03:10,  1.17s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 331/492 [06:14<03:09,  1.17s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 332/492 [06:15<03:08,  1.18s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 333/492 [06:16<03:06,  1.17s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 334/492 [06:18<03:06,  1.18s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 335/492 [06:19<03:05,  1.18s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 336/492 [06:20<03:03,  1.18s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 337/492 [06:21<03:02,  1.18s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 338/492 [06:22<03:00,  1.17s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 339/492 [06:23<02:59,  1.17s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 340/492 [06:25<02:58,  1.17s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 341/492 [06:26<02:57,  1.17s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 342/492 [06:27<02:55,  1.17s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 343/492 [06:28<02:54,  1.17s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 344/492 [06:29<02:53,  1.17s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 345/492 [06:30<02:52,  1.17s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 346/492 [06:32<02:51,  1.18s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 347/492 [06:33<02:49,  1.17s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 348/492 [06:34<02:48,  1.17s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 349/492 [06:35<02:47,  1.17s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 350/492 [06:36<02:46,  1.17s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 351/492 [06:37<02:45,  1.17s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 352/492 [06:39<02:43,  1.17s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 353/492 [06:40<02:43,  1.17s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 354/492 [06:41<02:41,  1.17s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 355/492 [06:42<02:40,  1.17s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 356/492 [06:43<02:39,  1.17s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 357/492 [06:45<02:38,  1.18s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 358/492 [06:46<02:38,  1.18s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 359/492 [06:47<02:37,  1.19s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 360/492 [06:48<02:36,  1.18s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 361/492 [06:49<02:34,  1.18s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 362/492 [06:50<02:33,  1.18s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 363/492 [06:52<02:31,  1.18s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 364/492 [06:53<02:31,  1.18s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 365/492 [06:54<02:30,  1.18s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 366/492 [06:55<02:29,  1.18s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 367/492 [06:56<02:27,  1.18s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 368/492 [06:58<02:26,  1.18s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 369/492 [06:59<02:25,  1.18s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 370/492 [07:00<02:24,  1.19s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 371/492 [07:01<02:23,  1.18s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 372/492 [07:02<02:21,  1.18s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 373/492 [07:03<02:20,  1.18s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 374/492 [07:05<02:19,  1.18s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 375/492 [07:06<02:18,  1.19s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 376/492 [07:07<02:16,  1.18s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 377/492 [07:08<02:15,  1.18s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 378/492 [07:09<02:14,  1.18s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 379/492 [07:11<02:13,  1.19s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 380/492 [07:12<02:12,  1.19s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 381/492 [07:13<02:10,  1.18s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 382/492 [07:14<02:09,  1.17s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 383/492 [07:15<02:08,  1.18s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 384/492 [07:16<02:07,  1.18s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 385/492 [07:18<02:06,  1.18s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 386/492 [07:19<02:05,  1.18s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 387/492 [07:20<02:03,  1.18s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 388/492 [07:21<02:02,  1.18s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 389/492 [07:22<02:01,  1.18s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 390/492 [07:24<02:00,  1.18s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 391/492 [07:25<01:59,  1.18s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 392/492 [07:26<01:58,  1.18s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 393/492 [07:27<01:57,  1.18s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 394/492 [07:28<01:55,  1.18s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 395/492 [07:29<01:54,  1.18s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 396/492 [07:31<01:53,  1.18s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 397/492 [07:32<01:52,  1.18s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 398/492 [07:33<01:50,  1.18s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 399/492 [07:34<01:49,  1.18s/it]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 400/492 [07:35<01:48,  1.18s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 401/492 [07:37<01:47,  1.18s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 402/492 [07:38<01:45,  1.18s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 403/492 [07:39<01:45,  1.18s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 404/492 [07:40<01:44,  1.19s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 405/492 [07:41<01:43,  1.18s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 406/492 [07:42<01:41,  1.18s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 407/492 [07:44<01:39,  1.18s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 408/492 [07:45<01:38,  1.18s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 409/492 [07:46<01:37,  1.18s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 410/492 [07:47<01:36,  1.18s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 411/492 [07:48<01:35,  1.18s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 412/492 [07:50<01:34,  1.18s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 413/492 [07:51<01:33,  1.18s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 414/492 [07:52<01:32,  1.18s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 415/492 [07:53<01:30,  1.18s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 416/492 [07:54<01:29,  1.18s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 417/492 [07:55<01:28,  1.18s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 418/492 [07:57<01:27,  1.18s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 419/492 [07:58<01:26,  1.18s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 420/492 [07:59<01:24,  1.18s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 421/492 [08:00<01:23,  1.18s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 422/492 [08:01<01:22,  1.18s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 423/492 [08:03<01:21,  1.19s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 424/492 [08:04<01:20,  1.19s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 425/492 [08:05<01:19,  1.19s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 426/492 [08:06<01:18,  1.19s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 427/492 [08:07<01:16,  1.18s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 428/492 [08:08<01:15,  1.18s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 429/492 [08:10<01:14,  1.19s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 430/492 [08:11<01:13,  1.19s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 431/492 [08:12<01:11,  1.18s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 432/492 [08:13<01:10,  1.18s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 433/492 [08:14<01:09,  1.18s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 434/492 [08:16<01:08,  1.18s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 435/492 [08:17<01:07,  1.19s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 436/492 [08:18<01:06,  1.18s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 437/492 [08:19<01:05,  1.18s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 438/492 [08:20<01:03,  1.18s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 439/492 [08:21<01:02,  1.18s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 440/492 [08:23<01:01,  1.18s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 441/492 [08:24<01:00,  1.18s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 442/492 [08:25<00:58,  1.18s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 443/492 [08:26<00:57,  1.18s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 444/492 [08:27<00:56,  1.18s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 445/492 [08:29<00:55,  1.19s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 446/492 [08:30<00:54,  1.18s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 447/492 [08:31<00:53,  1.19s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 448/492 [08:32<00:52,  1.18s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 449/492 [08:33<00:51,  1.19s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 450/492 [08:34<00:49,  1.19s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 451/492 [08:36<00:48,  1.19s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 452/492 [08:37<00:47,  1.19s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 453/492 [08:38<00:46,  1.19s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 454/492 [08:39<00:45,  1.19s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 455/492 [08:40<00:43,  1.18s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 456/492 [08:42<00:42,  1.19s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 457/492 [08:43<00:41,  1.19s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 458/492 [08:44<00:40,  1.19s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 459/492 [08:45<00:39,  1.19s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 460/492 [08:46<00:38,  1.19s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 461/492 [08:48<00:36,  1.19s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 462/492 [08:49<00:35,  1.19s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 463/492 [08:50<00:34,  1.18s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 464/492 [08:51<00:32,  1.18s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 465/492 [08:52<00:31,  1.18s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 466/492 [08:53<00:30,  1.19s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 467/492 [08:55<00:29,  1.19s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 468/492 [08:56<00:28,  1.18s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 469/492 [08:57<00:27,  1.18s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 470/492 [08:58<00:26,  1.18s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 471/492 [08:59<00:24,  1.18s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 472/492 [09:01<00:23,  1.18s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 473/492 [09:02<00:22,  1.18s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 474/492 [09:03<00:21,  1.18s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 475/492 [09:04<00:20,  1.18s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 476/492 [09:05<00:18,  1.18s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 477/492 [09:06<00:17,  1.18s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 478/492 [09:08<00:16,  1.18s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 479/492 [09:09<00:15,  1.18s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 480/492 [09:10<00:14,  1.18s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 481/492 [09:11<00:12,  1.18s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 482/492 [09:12<00:11,  1.18s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 483/492 [09:14<00:10,  1.18s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 484/492 [09:15<00:09,  1.18s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 485/492 [09:16<00:08,  1.18s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 486/492 [09:17<00:07,  1.18s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 487/492 [09:18<00:05,  1.18s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 488/492 [09:19<00:04,  1.18s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 489/492 [09:21<00:03,  1.18s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 490/492 [09:22<00:02,  1.18s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 491/492 [09:23<00:01,  1.19s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 492/492 [09:23<00:00,  1.11it/s]\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 737\u001b[0m\n",
      "\u001b[34mNum examples = 737\n",
      "  Batch size = 64\u001b[0m\n",
      "\u001b[34mBatch size = 64\u001b[0m\n",
      "\u001b[34m0%|          | 0/12 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 2/12 [00:02<00:10,  1.07s/it]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 3/12 [00:04<00:13,  1.51s/it]#033[A\u001b[0m\n",
      "\u001b[34m33%|███▎      | 4/12 [00:06<00:13,  1.74s/it]#033[A\u001b[0m\n",
      "\u001b[34m42%|████▏     | 5/12 [00:08<00:13,  1.88s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 6/12 [00:10<00:11,  1.96s/it]#033[A\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 7/12 [00:12<00:10,  2.02s/it]#033[A\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 8/12 [00:14<00:08,  2.05s/it]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 9/12 [00:17<00:06,  2.08s/it]#033[A\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 10/12 [00:19<00:04,  2.10s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 11/12 [00:21<00:02,  2.11s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 12/12 [00:22<00:00,  1.80s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.30263540148735046, 'eval_accuracy': 0.903663500678426, 'eval_runtime': 24.5705, 'eval_samples_per_second': 29.995, 'eval_steps_per_second': 0.488, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 492/492 [09:48<00:00,  1.11it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 12/12 [00:22<00:00,  1.80s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m{'train_runtime': 588.3591, 'train_samples_per_second': 10.018, 'train_steps_per_second': 0.836, 'train_loss': 0.7223663950354103, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 492/492 [09:48<00:00,  1.11it/s]#015100%|██████████| 492/492 [09:48<00:00,  1.20s/it]\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 737\u001b[0m\n",
      "\u001b[34mNum examples = 737\n",
      "  Batch size = 64\u001b[0m\n",
      "\u001b[34mBatch size = 64\u001b[0m\n",
      "\u001b[34m0%|          | 0/12 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 2/12 [00:02<00:10,  1.07s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 3/12 [00:04<00:13,  1.51s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 4/12 [00:06<00:13,  1.75s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 5/12 [00:08<00:13,  1.88s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 6/12 [00:10<00:11,  1.97s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 7/12 [00:12<00:10,  2.02s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 8/12 [00:14<00:08,  2.05s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 9/12 [00:17<00:06,  2.08s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 10/12 [00:19<00:04,  2.09s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 11/12 [00:21<00:02,  2.10s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 12/12 [00:22<00:00,  1.80s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 12/12 [00:22<00:00,  1.87s/it]\u001b[0m\n",
      "\u001b[34m***** Eval results *****\u001b[0m\n",
      "\u001b[34mepoch = 1.0\u001b[0m\n",
      "\u001b[34meval_accuracy = 0.903663500678426\u001b[0m\n",
      "\u001b[34meval_loss = 0.30263540148735046\u001b[0m\n",
      "\u001b[34meval_runtime = 24.5672\u001b[0m\n",
      "\u001b[34meval_samples_per_second = 29.999\u001b[0m\n",
      "\u001b[34meval_steps_per_second = 0.488\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m2022-11-30 10:21:23,367 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2022-11-30 10:21:23,367 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2022-11-30 10:21:23,368 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-11-30 10:22:17 Uploading - Uploading generated training model\n",
      "2022-11-30 10:23:18 Completed - Training job completed\n",
      "Training seconds: 920\n",
      "Billable seconds: 920\n",
      "ジョブの名称: huggingface-pytorch-training-2022-11-30-10-05-45-257\n"
     ]
    }
   ],
   "source": [
    "# 使用するデータを指定して訓練を始める\n",
    "huggingface_estimator.fit({\n",
    "    'train': train_input_path,  # 訓練用のデータ\n",
    "    'test': test_input_path,    # 精度確認用のデータ\n",
    "})\n",
    "\n",
    "# 後から参照するため訓練ジョブの名称を控えておく\n",
    "job_name = huggingface_estimator.latest_training_job.job_name\n",
    "print(\"ジョブの名称:\", job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a1c962-b9a5-4f22-9406-e0aff56883fe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 訓練用リソースについて"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94412225-d79c-4aba-8ff5-35b231c334b1",
   "metadata": {},
   "source": [
    "上で訓練用ジョブを定義する際、`source_dir = \"s3://...\"` という引数で訓練用リソースを渡していました。\n",
    "\n",
    "この s3 arn が指しているのは、ハンズオンの最初にダウンロードした `sourcedir.tar.gz` と同じもの（つまりノートブック以外のファイル全て）です。\n",
    "\n",
    "訓練の間にこれらについて内容を説明していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41442107-9909-427d-96b6-15bbaf42d22b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### チューニング結果の確認"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bcc2ce-6a92-49c7-b3b8-5875c57e7b9d",
   "metadata": {},
   "source": [
    "訓練の結果、最終的にどの程度の精度になったのかを確認します。\n",
    "\n",
    "ログの最後の方、\"***** Eval results *****\" 以降にファインチューニング後のモデル評価結果が出力されています。\n",
    "`eval_accuracy = ...` の部分がテストデータでの精度です。\n",
    "学習には乱数が関わるため一定ではありませんが、およそ 90%の精度になっているかと思います。\n",
    "\n",
    "今回のタスクには９つのラベルがありました。\n",
    "ファインチューニングなしのモデルでは出力ラベルがランダムなため、精度は 1/9、すなわち11%程度となります。\n",
    "これを踏まえると、ファインチューニングによってモデルがタスクに適応し、高い精度が得られるようになっていることがわかります。\n",
    "\n",
    "なお今回は訓練データ一周分 (1 epoch) の訓練を行いましたが、この量やそのほかの学習パラメータを調整することで更に高い精度が得られる可能性もあります。\n",
    "この、タスクに最適なパラメータを探す作業をハイパーパラメータ探索と呼びますが、本ハンズオンでは詳細は割愛します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865cd683-6bd8-4fa3-ab49-dab813a5aedd",
   "metadata": {
    "id": "582d21fc-7e06-4e63-95a3-3d9e73673570",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 訓練したモデルのデプロイ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d55c44-cd28-494b-a98f-4dd22357f3fc",
   "metadata": {
    "id": "582d21fc-7e06-4e63-95a3-3d9e73673570",
    "tags": []
   },
   "source": [
    "最後に、先ほど訓練したモデルをデプロイして、実際にテキストのラベルを予測させてみましょう。\n",
    "\n",
    "こちらも訓練の時と同様、SageMakerのAPIを呼ぶ形になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c8e1ec-9780-4f59-8312-2b1a583eb1ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### SageMakerでモデルをデプロイする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7380276e-86cb-4255-8ade-7bedc7217152",
   "metadata": {
    "id": "7380276e-86cb-4255-8ade-7bedc7217152",
    "outputId": "36279dc7-6429-4322-8d68-b9b47b1c73be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2022-11-30 10:23:34 Starting - Preparing the instances for training\n",
      "2022-11-30 10:23:34 Downloading - Downloading input data\n",
      "2022-11-30 10:23:34 Training - Training image download completed. Training in progress.\n",
      "2022-11-30 10:23:34 Uploading - Uploading generated training model\n",
      "2022-11-30 10:23:34 Completed - Training job completed\n",
      "-----------!"
     ]
    }
   ],
   "source": [
    "# 訓練ジョブの名称でデプロイする訓練済みモデルを指定\n",
    "# もしくは上の `huggingface_estimator` をそのまま使うことも可能\n",
    "estimator = HuggingFace.attach(job_name, sagemaker_session=sess)\n",
    "\n",
    "# モデルをデプロイする\n",
    "predictor = estimator.deploy(1, \"ml.g4dn.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aad4d47-0b3f-4693-9527-74acb040f5cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### ラベルを予測させる\n",
    "タスクデータからいくつかサンプルをとって、ラベルを予測させてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7485ddb1-297f-4a28-84dd-07281489dc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ラベルごとの予測スコアを表示する関数\n",
    "def print_label_score(pred):\n",
    "    print(\"各ラベルのスコア:\")\n",
    "    for l, score in sorted(enumerate(pred[\"logits\"][0]), key=lambda x: -x[1]):\n",
    "        print(f\"{id2label[l]}\\t\", f\"{score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "511cc094-8410-4b25-a3ab-46fae3023962",
   "metadata": {
    "id": "de412e3a-7693-4b51-a46a-197294ae55ee",
    "outputId": "d74139e2-f8bb-4e49-f896-8118590d3d48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "真のラベル: sports-watch\n",
      "本文: 野球解説者、野村克也氏が今年の野球界を漢字一文字で表すと？ TBSのスポーツ番組「S1」（25日深夜放送）も年内最後の放送となり、人気コーナー「ノムさんぼやき部屋」もこの日の放送でボヤキおさめとなった。そんな今年最後のテーマは前述した通り——。 登場するや、TBS枡田絵理奈アナウンサーに「誕生日おめでとう」と花束を渡したノムさん。12月25日で26歳になった枡田アナに「36歳か？」というボケをかま...\n",
      "\n",
      "予測ラベル: sports-watch\n",
      "各ラベルのスコア:\n",
      "sports-watch\t 4.21\n",
      "topic-news\t 4.06\n",
      "it-life-hack\t 0.21\n",
      "livedoor-homme\t -0.58\n",
      "movie-enter\t -0.64\n",
      "kaden-channel\t -0.93\n",
      "smax\t -1.23\n",
      "peachy\t -1.63\n",
      "dokujo-tsushin\t -2.08\n"
     ]
    }
   ],
   "source": [
    "# ランダムなデータを選択\n",
    "d = livedoor_data.shuffle()[0]\n",
    "text = d[\"sentence1\"]\n",
    "label = d[\"label\"]\n",
    "\n",
    "print(\"真のラベル:\", label)\n",
    "print(f\"本文: {text[:200]}...\\n\")\n",
    "\n",
    "# モデルにラベルを予測させる\n",
    "pred = predictor.predict({\"inputs\": text})\n",
    "\n",
    "print(\"予測ラベル:\", id2label[pred[\"label\"]])\n",
    "print_label_score(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ddfbb56-9d20-4134-b345-e5af29fe9ea8",
   "metadata": {
    "id": "de412e3a-7693-4b51-a46a-197294ae55ee",
    "outputId": "d74139e2-f8bb-4e49-f896-8118590d3d48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "予測ラベル: dokujo-tsushin\n",
      "各ラベルのスコア:\n",
      "dokujo-tsushin\t 1.83\n",
      "kaden-channel\t 0.76\n",
      "livedoor-homme\t 0.59\n",
      "it-life-hack\t 0.20\n",
      "sports-watch\t -0.24\n",
      "movie-enter\t -0.29\n",
      "smax\t -0.48\n",
      "topic-news\t -1.04\n",
      "peachy\t -1.12\n"
     ]
    }
   ],
   "source": [
    "# 新規の文章を入力とする\n",
    "text = \"吾輩は猫である。\"\n",
    "\n",
    "# モデルにラベルを予測させる\n",
    "pred = predictor.predict({\"inputs\": text})\n",
    "\n",
    "print(\"予測ラベル:\", id2label[pred[\"label\"]])\n",
    "print_label_score(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad7acee-a760-481b-bd0f-a599b01dbe3a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 後処理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d51bdfc-b160-4277-bbcd-db663e1b2e73",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "最後にデプロイしたインスタンスを削除します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d99bf1c8-c44e-442a-95b0-96668fc695fa",
   "metadata": {
    "id": "d99bf1c8-c44e-442a-95b0-96668fc695fa"
   },
   "outputs": [],
   "source": [
    "# 間違えて削除してしまわないようコメントアウトしている\n",
    "# 行頭の \"#\" を削除してから実行する\n",
    "\n",
    "# predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
