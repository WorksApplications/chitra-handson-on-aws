{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b34e3602-be30-4320-98bb-a4e67fa175e8",
   "metadata": {
    "id": "VX-6Z_MfIR1T",
    "tags": []
   },
   "source": [
    "# chiTraãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dfe4cb-ce55-4ebc-af43-19be0ddf0d03",
   "metadata": {
    "id": "VX-6Z_MfIR1T",
    "tags": []
   },
   "source": [
    "å‰ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§è¦‹ãŸã‚ˆã†ã«ã€é…å¸ƒã•ã‚Œã¦ã„ã‚‹chiTraãƒ¢ãƒ‡ãƒ«ã¯ã€Œãƒã‚¹ã‚¯ã•ã‚ŒãŸèªã‚’äºˆæ¸¬ã™ã‚‹ã€ã‚ˆã†ã«å­¦ç¿’ã•ã‚ŒãŸçŠ¶æ…‹ã§ã€ãã®ã¾ã¾ã§ã¯ä»–ã®ã‚¿ã‚¹ã‚¯ã«ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚\n",
    "ãã“ã§ã€å¿œç”¨ã—ãŸã„ã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦æœ›ã‚€å‡ºåŠ›ãŒã§ãã‚‹ã‚ˆã†èª¿æ•´ã‚’è¡Œã„ã¾ã™ã€‚ã“ã‚ŒãŒãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨å‘¼ã°ã‚Œã‚‹ä½œæ¥­ã§ã™ã€‚\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€\n",
    "- chiTraãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿéš›ã®ã‚¿ã‚¹ã‚¯ã«åˆã‚ã›ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€ãã®åŠ¹æœã‚’ç¢ºèªã™ã‚‹ã“ã¨\n",
    "- è¨“ç·´ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã€å®Ÿéš›ã«ä½¿ã£ã¦ã¿ã‚‹ã“ã¨\n",
    "\n",
    "ãŒã‚´ãƒ¼ãƒ«ã§ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb11ea6-cb23-4a48-aa2e-504714c449c6",
   "metadata": {
    "id": "ZDS2ZaefIb7U",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## æº–å‚™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c663b59f-0de4-4f0e-961f-80a75eb61713",
   "metadata": {},
   "source": [
    "### å‡ºåŠ›ã®æ¶ˆå»\n",
    "é…å¸ƒã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€å„ã‚»ãƒ«ã®å®Ÿè¡Œçµæœã‚’å‚ç…§ç”¨ã«æ®‹ã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "ä½œæ¥­ã«ãŠã„ã¦ã¯å®Ÿè¡Œå ´æ‰€ãŒã‚ã‹ã‚Šã«ãããªã‚‹ã®ã§ã€å³ã‚¯ãƒªãƒƒã‚¯ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰`Clear All Outputs`ã‚’å®Ÿè¡Œã—ã¦æ¶ˆå»ã—ã¾ã™ã€‚\n",
    "\n",
    "æœ¬æ¥ã©ã®ã‚ˆã†ã«ãªã‚‹ã®ã‹ç¢ºèªã—ãŸã„ã€åˆæœŸçŠ¶æ…‹ã«æˆ»ã—ãŸã„ãªã©ã®å ´åˆã¯ã€\n",
    "ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‹ã‚‰ã‚³ãƒãƒ³ãƒ‰ `tar -xvf notebooks.tar.gz` ã§å†åº¦å±•é–‹ã‚’è¡Œã£ã¦ãã ã•ã„ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸Šæ›¸ãã«ã”æ³¨æ„ãã ã•ã„ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93625c24-6f31-4ece-93b0-db12a62d3a75",
   "metadata": {},
   "source": [
    "### å®šæ•°\n",
    "chiTraãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ã¸ã®ãƒ‘ã‚¹ã®ã»ã‹ã€AWSå†…ã®ãƒªã‚½ãƒ¼ã‚¹ã¸ã®ãƒ‘ã‚¹ãªã©ã€\n",
    "ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å†…ã§ä½¿ç”¨ã™ã‚‹å®šæ•°ã‚’ã„ãã¤ã‹å®šç¾©ã—ã¾ã™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e427ab-6c88-408c-be0a-3218d9958049",
   "metadata": {},
   "outputs": [],
   "source": [
    "chitra_path = \"./chiTra-1.0\"\n",
    "\n",
    "s3_handson_bucket = \"chitra-handson-20221203\"\n",
    "s3_source_path = f\"s3://{s3_handson_bucket}/source/sourcedir.tar.gz\"\n",
    "s3_data_path = f\"s3://{s3_handson_bucket}/datasets/livedoor\"\n",
    "s3_output_path = f\"s3://{s3_handson_bucket}/trained\"\n",
    "\n",
    "rawdata_path = f\"{s3_data_path}/raw\"\n",
    "train_input_path = f\"{s3_data_path}/train\"\n",
    "test_input_path = f\"{s3_data_path}/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8196de0f-90d7-4ecf-9148-23d8ed60fce8",
   "metadata": {
    "id": "-23O_0VYcx1i",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8014aca8-d684-4d8a-9c0a-5ea94d72d6ec",
   "metadata": {
    "id": "-23O_0VYcx1i",
    "tags": []
   },
   "source": [
    "ä»Šå›ã¯ [livedoor ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‘ã‚¹](https://www.rondhuit.com/download.html#ldcc)ã‚’åˆ©ç”¨ã—ã¾ã™ã€‚\n",
    "ã“ã¡ã‚‰ã¯ã€Œlivedoor ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€ã®ï¼™ã¤ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰è¨˜äº‹ã‚’åé›†ã—ãŸã‚‚ã®ã§ã™ã€‚\n",
    "\n",
    "è¨˜äº‹æœ¬æ–‡ã®å†…å®¹ã‚’å…¥åŠ›ã¨ã—ã¦ã€ã©ã®ã‚«ãƒ†ã‚´ãƒªã®è¨˜äº‹ã‹ã‚’åˆ¤å®šã™ã‚‹ã‚¿ã‚¹ã‚¯ã‚’å¯¾è±¡ã¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã„ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a109ec6-f655-45b8-a829-4792ef95335e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f34cc84-5d64-4ab9-a723-543b86cbf03b",
   "metadata": {
    "id": "-23O_0VYcx1i",
    "tags": []
   },
   "source": [
    "ã¾ãšã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å†…å®¹ã‚’å°‘ã—è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7c5eac0-8924-4d7e-ab4e-451c2cfd1eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "import datasets\n",
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()\n",
    "livedoor_data = datasets.load_from_disk(rawdata_path, fs=s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c2223da-7690-4d09-b06b-a98834506ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence1', 'label'],\n",
      "    num_rows: 7367\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã®é‡ã‚’ç¢ºèª\n",
    "print(livedoor_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95370667-a499-42c8-aeac-ef6eb2cdeb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ãƒ©ãƒ™ãƒ«ä¸€è¦§ ['dokujo-tsushin', 'it-life-hack', 'kaden-channel', 'livedoor-homme', 'movie-enter', 'peachy', 'smax', 'sports-watch', 'topic-news']\n"
     ]
    }
   ],
   "source": [
    "# ã‚«ãƒ†ã‚´ãƒªãƒ©ãƒ™ãƒ«ã®ä¸€è¦§\n",
    "labels = list(sorted(set(livedoor_data[\"label\"])))\n",
    "print(\"ãƒ©ãƒ™ãƒ«ä¸€è¦§\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f69e145-5c4f-4799-b00f-8837ebf7b1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ãƒ©ãƒ™ãƒ«: it-life-hack\n",
      "æœ¬æ–‡: æˆ»ã£ã¦ã¾ã„ã‚Šã¾ã—ãŸï¼å…ˆæ—¥ã‚ãˆãªãç •ã‘æ•£ã£ãŸã‚¤ãƒ³ãƒ†ãƒ«ã®æœ€æ–°SSDã€Œ520ã‚·ãƒªãƒ¼ã‚ºã€ã‚’æ—§å‹Macã«å–ã‚Šã¤ã‘ã‚‹ã¨ã„ã†é€£è¼‰ã€‚ä»Šå›ã‹ã‚‰è£…ã„ã‚‚æ–°ãŸã«ã€ã‹ã¤åèª‰æŒ½å›ã‚’ç›®æŒ‡ã—å‰å›ã¨ã¯ç•°ãªã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§è£…ç€ã•ã›ã‚‹ã“ã¨ã‚’è©¦ã¿ãŸã€‚æ—§MacãŒSSDã«ã‚ˆã‚‹ã€Œå¿«é©å‹•ä½œã€ã«æˆåŠŸã™ã‚‹ã‹ã©ã†ã‹ã€ä¹ã†ã”æœŸå¾…ã€‚ â– SSDå¤–ä»˜ã‘ã§ã®èµ·å‹•ã«ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ ä»¥å‰ã®ã€Œã‚¤ãƒ³ãƒ†ãƒ« SSD 520ã‚’æ—§Macã«è£…ç€ã€ã‚·ãƒªãƒ¼ã‚ºã¯ã€çµå±€ã€MacProã§ã®å‹•ä½œã«æˆåŠŸã—ãªã„ã¾ã¾çµ‚äº†ã—ã¦ã—ã¾ã£ãŸã€‚ç­†è€…ã¯SATAã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ã«å•é¡ŒãŒã‚ã‚‹ã¨æ¨æ¸¬ã—ãŸãŒã€ç™ºå£²å¾Œï¼•å¹´ä»¥ä¸Šã‚’çµŒãŸä»Šã€ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ã®æ”¹å–„ã¯æœŸå¾…ã§ããªã„ã€‚ãã“ã§ã€SSDã‚’æ—¢å­˜ã®HDDã‚¹ãƒšãƒ¼ã‚¹ã«è£…ç€ã™ã‚‹ä»¥å¤–ã®æ–¹æ³•ã‚’è©¦ã—ã¦ã¿ã‚‹ã“ã¨ã«ã—ãŸã€‚ Macã¯ã€Windowsã¨ç•°ãªã‚Šã€å¤–ä»˜ã‘æ©Ÿå™¨ã‹ã‚‰ã®èµ·å‹•ã‚’åºƒãã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹ã€‚æ¨™æº–ã§ã€USBæ¥ç¶šã®å¤–ä»˜ã‘HDDã‚„ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ãƒ¡ãƒ¢ãƒªã€FireWireï¼ˆIEEE1394ï¼‰æ¥ç¶šã®å¤–...\n"
     ]
    }
   ],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«è¡¨ç¤º\n",
    "data = livedoor_data.shuffle()[0]\n",
    "\n",
    "print(\"ãƒ©ãƒ™ãƒ«:\", data['label'])\n",
    "print(\"æœ¬æ–‡:\", f\"{data['sentence1'][:400]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4b2d8c-5d04-405f-9db4-e317e5970665",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### ãƒ‡ãƒ¼ã‚¿ã®åŠ å·¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86c06a9-5f41-4e78-adb6-a61edb2c81b5",
   "metadata": {},
   "source": [
    "å‰ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§è¦‹ãŸã‚ˆã†ã«ã€chiTraãƒ¢ãƒ‡ãƒ«ã¯ãƒ†ã‚­ã‚¹ãƒˆã‚’ãã®ã¾ã¾æ‰±ã†ã“ã¨ã¯ã§ããªã„ã®ã§ã€chiTraãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’ä½¿ã£ã¦åŠ å·¥ã‚’è¡Œã„ã¾ã™ã€‚\n",
    "\n",
    "ã¾ãŸä½µã›ã¦ã‚«ãƒ†ã‚´ãƒªãƒ©ãƒ™ãƒ«ã«ã¤ã„ã¦ã‚‚æ•°å€¤ã¸å¤‰æ›ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95e82d7f-c0d8-42c3-8a82-6b7b0f8cc288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee09612d796c450587bc9b702d376545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sudachitra as chitra\n",
    "\n",
    "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’èª­ã¿è¾¼ã‚€\n",
    "tok = chitra.BertSudachipyTokenizer.from_pretrained(chitra_path)\n",
    "\n",
    "# ãƒ©ãƒ™ãƒ«ã‚’æ•°å€¤ã«å¤‰æ›ãƒ»é€†å¤‰æ›ã™ã‚‹\n",
    "label2id = {l:i for i,l in enumerate(labels)}\n",
    "id2label = {i:l for i,l in enumerate(labels)}\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¢ãƒ‡ãƒ«å…¥åŠ›ç”¨ã«æ•´å½¢ã™ã‚‹é–¢æ•°\n",
    "def preprocess(data):\n",
    "    # chiTraãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’é©ç”¨\n",
    "    ret = tok(data[\"sentence1\"], padding=True, truncation=True, max_length=512)\n",
    "    # ãƒ©ãƒ™ãƒ«ã‚’å¤‰æ›\n",
    "    ret[\"label\"] = [label2id[l] for l in data[\"label\"]]\n",
    "    return ret\n",
    "\n",
    "# åŠ å·¥ã®å®Ÿè¡Œï¼ˆã“ã“ã§ã¯ä¸€éƒ¨ã®ã¿ï¼‰\n",
    "processed_data = livedoor_data.shuffle().select(range(100)).map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=[\"sentence1\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a144f74b-df5c-4cb6-bb87-36791f989655",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 1, 'input_ids': [2, 19559, 18763, 484, 10309, 1432, 16694, 2476, 485, 418, 16295, 519, 11254, 476, 14922, 480, 12116, 519, 461, 476, 10146, 419, 12861, 1083, 6514, 484, 11316, 6139, 6740, 481, 10653, 11632, 450, 11098, 477, 12903, 478, 10152, 476, 10146, 419, 10209, 485, 418, 13962, 484, 1106, 5388, 10148, 11499, 469, 419, 1432, 11476, 485, 418, 427, 23905, 11477, 23488, 22031, 484, 10992, 11791, 477, 418, 19559, 484, 11316, 6139, 6740, 484, 12370, 12244, 459, 514, 469, 12610, 450, 11187, 459, 514, 476, 442, 11439, 419, 27044, 12135, 11370, 450, 15154, 481, 12244, 461, 11439, 419, 63, 30620, 5305, 6507, 7312, 7312, 69, 5346, 5298, 5305, 16, 24995, 17, 63, 5300, 5537, 20, 29118, 428, 478, 418, 28064, 5810, 5497, 469, 419, 10154, 1106, 5388, 481, 10272, 476, 418, 20734, 484, 13307, 450, 14098, 10158, 476, 10146, 419, 427, 11847, 461, 476, 442, 11439, 428, 427, 693, 481, 616, 5649, 15574, 10811, 10152, 476, 504, 15621, 485, 1214, 481, 13242, 476, 781, 10400, 2052, 6852, 461, 476, 10715, 10148, 419, 2120, 10148, 11744, 15738, 15362, 419, 12914, 450, 10568, 11263, 477, 10874, 519, 19077, 481, 1, 428, 427, 10453, 481, 10699, 476, 16383, 484, 11553, 11791, 10336, 418, 23565, 3025, 480, 11791, 477, 4774, 5390, 480, 510, 51, 428, 427, 13962, 15457, 10389, 10145, 657, 504, 18799, 20890, 477, 12244, 10145, 11939, 461, 483, 1, 428, 427, 20522, 10148, 20740, 476, 22017, 469, 478, 23362, 5523, 16224, 450, 19416, 909, 476, 11295, 11939, 480, 419, 428, 427, 1717, 485, 418, 10983, 5423, 484, 11665, 6858, 450, 10665, 419, 428, 427, 1414, 10257, 448, 5590, 24468, 11, 1, 403, 1, 12, 484, 428, 427, 16373, 450, 19422, 10181, 469, 10151, 484, 3950, 11555, 477, 485, 1, 428, 427, 31252, 519, 17905, 481, 419, 428, 427, 20522, 1778, 461, 476, 10160, 428, 10604, 480, 11752, 450, 14098, 10158, 476, 10146, 419, 399, 1432, 16694, 11, 22996, 10810, 10359, 12, 11721, 16295, 399, 1432, 11476, 481, 11043, 461, 469, 11791, 519, 15618, 11654, 610, 13563, 484, 16245, 519, 11822, 11439, 6, 1432, 11476, 4981, 14156, 15282, 484, 1717, 481, 504, 2123, 485, 12431, 13888, 4981, 610, 16295, 484, 11086, 484, 2091, 450, 535, 14006, 5497, 5497, 6, 1432, 11476, 26102, 10323, 11632, 450, 12903, 610, 18594, 450, 477, 19676, 6, 1432, 11476, 11520, 12546, 10833, 481, 15069, 484, 1311, 610, 14108, 5950, 484, 15574, 10936, 504, 10811, 10176, 449, 480, 1, 1432, 11476, 2281, 6813, 3025, 480, 11039, 450, 12903, 610, 21404, 477, 485, 11786, 6, 1432, 11476, 16295, 14815, 484, 12116, 481, 18048, 1432, 484, 20, 645, 484, 17170, 1432, 16694, 484, 10570, 12153, 11, 19031, 5717, 13842, 12, 17028, 28, 30296, 3683, 5809, 19031, 5717, 11514, 11, 10463, 15, 18857, 15, 18061, 12, 10637, 5489, 28, 20041, 16, 18552, 5415, 5535, 5305, 1062, 24860, 519, 11654, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# åŠ å·¥æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«è¡¨ç¤º\n",
    "print(processed_data.shuffle()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace10b78-784b-457c-adc4-4bca513dc0ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### ãƒ‡ãƒ¼ã‚¿ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f76cf8-3e70-486a-8640-91da1b3e55a3",
   "metadata": {},
   "source": [
    "ä»Šå›ã¯æº–å‚™æ¸ˆã¿ãªã®ã§å¿…è¦ã‚ã‚Šã¾ã›ã‚“ãŒã€ä¸€ã‹ã‚‰æº–å‚™ã‚’è¡Œã†å ´åˆã¯ã•ã‚‰ã«ã€\n",
    "è¨“ç·´ç”¨ã¨ç¢ºèªç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã—ã€s3ã¸ã¨ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãŠãã“ã¨ã«ãªã‚Šã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c753dc83-7174-41a4-ba4b-ad3d58a1e77e",
   "metadata": {
    "id": "c753dc83-7174-41a4-ba4b-ad3d58a1e77e",
    "outputId": "62335baa-120b-4aa0-a766-eb579b25e791"
   },
   "outputs": [],
   "source": [
    "# import botocore\n",
    "# from datasets.filesystems import S3FileSystem\n",
    "\n",
    "# split = processed_data.train_test_split(test_size=0.2)\n",
    "\n",
    "# s3 = S3FileSystem()\n",
    "# split[\"train\"].save_to_disk(f\"{train_input_path}_tmp\", fs=s3)\n",
    "# split[\"test\"].save_to_disk(f\"{test_input_path}_tmp\", fs=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145ddff7-77c1-4aec-9509-c5a6486fa1e7",
   "metadata": {
    "id": "897b819e-56bd-4098-aaf3-24bc20966a8c",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4531e51-2f52-40ea-b818-039565c9b2b3",
   "metadata": {
    "id": "897b819e-56bd-4098-aaf3-24bc20966a8c",
    "tags": []
   },
   "source": [
    "æ¬¡ã«åŠ å·¥ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã†ã‚¸ãƒ§ãƒ–ã‚’å®Ÿè¡Œã—ã¦ã„ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc186af-c6c4-4846-9652-d4bfcfa37583",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### SageMakerã§è¨“ç·´ã‚’è¡Œã†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313a15f1-1da9-49f6-be43-6479103184f7",
   "metadata": {
    "id": "897b819e-56bd-4098-aaf3-24bc20966a8c",
    "tags": []
   },
   "source": [
    "SageMaker SDKãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‹ã‚‰è¨“ç·´ã‚¸ãƒ§ãƒ–ã‚’ä½œæˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
    "\n",
    "APIã‚’å©ãå½¢ã«ãªã‚‹ãŸã‚ã“ã“ã§ã®ã‚³ãƒ¼ãƒ‰ã¯ã‚·ãƒ³ãƒ—ãƒ«ã§ã€äº‹å‰ã«ç”¨æ„ã—ãŸãƒªã‚½ãƒ¼ã‚¹ã‚„è¨“ç·´ã®ç´°ã‹ã„è¨­å®šã‚’å¼•æ•°ã¨ã—ã¦æ¸¡ã—ã¦ã„ãã“ã¨ã«ãªã‚Šã¾ã™ã€‚\n",
    "\n",
    "å­¦ç¿’ã«ã¯å°‘ã€…æ™‚é–“ãŒã‹ã‹ã‚‹ã®ã§ã€ã¾ãšã¯ã‚¸ãƒ§ãƒ–ã‚’å®Ÿè¡Œã—ã¦ã—ã¾ã„ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "927ab306-b868-43aa-995a-7229a0364b50",
   "metadata": {
    "id": "b3c487ce-b78d-47a3-a2af-2fa779f2a773"
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# SageMakerã¸ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ç¢ºç«‹\n",
    "sess = sagemaker.Session(default_bucket=s3_handson_bucket)\n",
    "\n",
    "# è¨“ç·´ã‚¸ãƒ§ãƒ–ã®å®šç¾©\n",
    "huggingface_estimator = HuggingFace(\n",
    "    sagemaker_session=sess,          # ã‚»ãƒƒã‚·ãƒ§ãƒ³\n",
    "    output_path=s3_output_path,      # è¨“ç·´çµæœã®å‡ºåŠ›å…ˆ\n",
    "    source_dir=s3_source_path,       # è¨“ç·´ç”¨ãƒªã‚½ãƒ¼ã‚¹ã®ç½®ãå ´æ‰€ï¼ˆå¾Œã§èª¬æ˜ï¼‰\n",
    "    entry_point='train.py',          # è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®åç§°\n",
    "    instance_type='ml.g4dn.xlarge',  # å­¦ç¿’ã«ä½¿ç”¨ã™ã‚‹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹\n",
    "    instance_count=1,\n",
    "    role=\"arn:aws:iam::012345678901:role/ChitraHandsonSageMakerExecution\",\n",
    "    transformers_version='4.12',\n",
    "    pytorch_version='1.9',\n",
    "    py_version='py38',\n",
    "    hyperparameters = {\n",
    "        'num_labels': len(labels), # ä»Šå›ã®ã‚¿ã‚¹ã‚¯ã®ãƒ©ãƒ™ãƒ«æ•°\n",
    "        'epochs': 1,               # ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®é‡\n",
    "        'train-batch-size': 12,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "834f6868-f4a5-4c12-b331-fb86fd7ca1b9",
   "metadata": {
    "id": "834f6868-f4a5-4c12-b331-fb86fd7ca1b9",
    "outputId": "d59a935c-734d-4962-f484-ad3f223e80f1",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-30 10:05:46 Starting - Starting the training job...\n",
      "2022-11-30 10:06:09 Starting - Preparing the instances for trainingProfilerReport-1669802745: InProgress\n",
      "............\n",
      "2022-11-30 10:08:18 Downloading - Downloading input data\n",
      "2022-11-30 10:08:18 Training - Downloading the training image............\n",
      "2022-11-30 10:10:30 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-11-30 10:10:36,000 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-11-30 10:10:36,026 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-11-30 10:10:36,029 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-11-30 10:10:40,836 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting sudachitra\u001b[0m\n",
      "\u001b[34mDownloading SudachiTra-0.1.7.tar.gz (283 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 283.7/283.7 kB 26.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting datasets>=2.6\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.7.1-py3-none-any.whl (451 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 451.7/451.7 kB 62.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting logzero~=1.7.0\u001b[0m\n",
      "\u001b[34mDownloading logzero-1.7.0-py2.py3-none-any.whl (16 kB)\u001b[0m\n",
      "\u001b[34mCollecting progressbar2~=3.53.1\u001b[0m\n",
      "\u001b[34mDownloading progressbar2-3.53.3-py2.py3-none-any.whl (25 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers>=0.10.3 in /opt/conda/lib/python3.8/site-packages (from sudachitra->-r requirements.txt (line 1)) (0.10.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers>=4.6.1 in /opt/conda/lib/python3.8/site-packages (from sudachitra->-r requirements.txt (line 1)) (4.12.3)\u001b[0m\n",
      "\u001b[34mCollecting sudachipy>=0.6.2\u001b[0m\n",
      "\u001b[34mDownloading SudachiPy-0.6.6-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.2 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2.2/2.2 MB 106.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting sudachidict_core>=20210802\u001b[0m\n",
      "\u001b[34mDownloading SudachiDict-core-20221021.tar.gz (9.0 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7 in /opt/conda/lib/python3.8/site-packages (from datasets>=2.6->-r requirements.txt (line 4)) (0.3.5.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets>=2.6->-r requirements.txt (line 4)) (3.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets>=2.6->-r requirements.txt (line 4)) (1.22.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.8/site-packages (from datasets>=2.6->-r requirements.txt (line 4)) (2022.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=2.6->-r requirements.txt (line 4)) (2.27.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets>=2.6->-r requirements.txt (line 4)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=2.6->-r requirements.txt (line 4)) (6.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from datasets>=2.6->-r requirements.txt (line 4)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets>=2.6->-r requirements.txt (line 4)) (1.2.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets>=2.6->-r requirements.txt (line 4)) (4.64.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets>=2.6->-r requirements.txt (line 4)) (0.70.13)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets>=2.6->-r requirements.txt (line 4)) (21.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=2.6->-r requirements.txt (line 4)) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets>=2.6->-r requirements.txt (line 4)) (3.10.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets>=2.6->-r requirements.txt (line 4)) (3.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing<3,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets>=2.6->-r requirements.txt (line 4)) (2.4.7)\u001b[0m\n",
      "\u001b[34mCollecting python-utils>=2.3.0\u001b[0m\n",
      "\u001b[34mDownloading python_utils-3.4.5-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from progressbar2~=3.53.1->sudachitra->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=2.6->-r requirements.txt (line 4)) (2022.5.18.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=2.6->-r requirements.txt (line 4)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=2.6->-r requirements.txt (line 4)) (2.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=2.6->-r requirements.txt (line 4)) (1.26.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers>=4.6.1->sudachitra->-r requirements.txt (line 1)) (2022.6.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers>=4.6.1->sudachitra->-r requirements.txt (line 1)) (0.0.53)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=2.6->-r requirements.txt (line 4)) (21.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=2.6->-r requirements.txt (line 4)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=2.6->-r requirements.txt (line 4)) (6.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=2.6->-r requirements.txt (line 4)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=2.6->-r requirements.txt (line 4)) (1.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=2.6->-r requirements.txt (line 4)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets>=2.6->-r requirements.txt (line 4)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets>=2.6->-r requirements.txt (line 4)) (2021.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers>=4.6.1->sudachitra->-r requirements.txt (line 1)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers>=4.6.1->sudachitra->-r requirements.txt (line 1)) (8.1.3)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sudachitra, sudachidict_core\u001b[0m\n",
      "\u001b[34mBuilding wheel for sudachitra (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for sudachitra (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for sudachitra: filename=SudachiTra-0.1.7-py3-none-any.whl size=266086 sha256=ddbca621a0cb32b5660277d831e0b82743b775eb8cbfd9ce5cc02388ce49775d\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/97/ad/02/69094014ac4631f81c227d7cf059fd46c11df9e5bfd03c6f41\u001b[0m\n",
      "\u001b[34mBuilding wheel for sudachidict_core (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for sudachidict_core (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for sudachidict_core: filename=SudachiDict_core-20221021-py3-none-any.whl size=71574762 sha256=a92747767647c7b2b0d89e8cf638539fc26ec8c5fd83c60e02b245bf1074f3a5\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/ca/e3/ed/e78fecf6fd34349114d292242a16fc08d513fb32c2d9c5d786\u001b[0m\n",
      "\u001b[34mSuccessfully built sudachitra sudachidict_core\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sudachipy, logzero, sudachidict_core, python-utils, responses, progressbar2, sudachitra, datasets\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 1.15.1\u001b[0m\n",
      "\u001b[34mUninstalling datasets-1.15.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-1.15.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed datasets-2.7.1 logzero-1.7.0 progressbar2-3.53.3 python-utils-3.4.5 responses-0.18.0 sudachidict_core-20221021 sudachipy-0.6.6 sudachitra-0.1.7\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.1.2 -> 22.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2022-11-30 10:11:03,037 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2022-11-30 10:11:03,037 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2022-11-30 10:11:03,118 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1,\n",
      "        \"num_labels\": 9,\n",
      "        \"train-batch-size\": 12\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2022-11-30-10-05-45-257\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://chitra-handson-20221203/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"num_labels\":9,\"train-batch-size\":12}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://chitra-handson-20221203/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"num_labels\":9,\"train-batch-size\":12},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2022-11-30-10-05-45-257\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://chitra-handson-20221203/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--num_labels\",\"9\",\"--train-batch-size\",\"12\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_LABELS=9\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN-BATCH-SIZE=12\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python train.py --epochs 1 --num_labels 9 --train-batch-size 12\u001b[0m\n",
      "\u001b[34m2022-11-30 10:11:04,865 - __main__ - INFO -  loaded train_dataset length is: 5894\u001b[0m\n",
      "\u001b[34m2022-11-30 10:11:04,865 - __main__ - INFO -  loaded test_dataset length is: 737\u001b[0m\n",
      "\u001b[34mtrain.py:54: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 4.21kB [00:00, 4.86MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at ./chiTra-1.0 were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of BertForSequenceClassification were not initialized from the model checkpoint at ./chiTra-1.0 and are newly initialized: ['classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at ./chiTra-1.0 were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of BertForSequenceClassification were not initialized from the model checkpoint at ./chiTra-1.0 and are newly initialized: ['classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m***** Running training *****\n",
      "  Num examples = 5894\n",
      "  Num Epochs = 1\u001b[0m\n",
      "\u001b[34mInstantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m***** Running training *****\n",
      "  Num examples = 5894\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34mTotal optimization steps = 492\u001b[0m\n",
      "\u001b[34mTotal optimization steps = 492\u001b[0m\n",
      "\u001b[34m0%|          | 0/492 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.474 algo-1:66 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.503 algo-1:66 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.504 algo-1:66 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.504 algo-1:66 INFO hook.py:200] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.505 algo-1:66 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.505 algo-1:66 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.591 algo-1:66 INFO hook.py:591] name:bert.embeddings.word_embeddings.weight count_params:25048320\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.591 algo-1:66 INFO hook.py:591] name:bert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.591 algo-1:66 INFO hook.py:591] name:bert.embeddings.token_type_embeddings.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.591 algo-1:66 INFO hook.py:591] name:bert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.591 algo-1:66 INFO hook.py:591] name:bert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.591 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.591 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.591 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.591 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.591 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.591 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.591 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.0.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.0.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.0.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.0.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.0.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.0.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.0.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.0.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.1.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.1.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.1.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.1.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.592 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.1.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.1.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.1.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.1.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.2.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.2.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.2.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.2.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.2.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.2.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.2.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.2.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.593 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.3.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.3.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.3.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.3.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.3.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.3.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.3.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.3.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.594 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.4.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.4.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.4.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.4.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.4.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.4.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.4.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.4.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.5.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.5.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.5.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.5.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.595 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.5.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.5.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.5.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.5.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.6.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.6.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.6.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.6.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.6.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.6.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.6.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.6.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.596 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.7.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.7.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.7.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.7.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.7.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.7.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.7.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.7.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.8.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.8.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.597 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.8.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.8.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.8.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.8.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.8.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.8.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.9.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.9.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.9.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.9.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.9.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.9.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.9.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.9.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.598 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.10.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.10.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.10.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.10.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.10.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.10.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.10.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.10.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.11.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.599 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.11.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.600 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.600 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.600 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.11.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.600 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.11.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.600 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.11.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.600 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.11.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.600 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.11.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.600 algo-1:66 INFO hook.py:591] name:bert.encoder.layer.11.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.600 algo-1:66 INFO hook.py:591] name:bert.pooler.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.600 algo-1:66 INFO hook.py:591] name:bert.pooler.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.600 algo-1:66 INFO hook.py:591] name:classifier.weight count_params:6912\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.600 algo-1:66 INFO hook.py:591] name:classifier.bias count_params:9\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.600 algo-1:66 INFO hook.py:593] Total Trainable Params: 111096585\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.600 algo-1:66 INFO hook.py:424] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-11-30 10:11:09.601 algo-1:66 INFO hook.py:488] Hook is writing from the hook with pid: 66\u001b[0m\n",
      "\u001b[34m0%|          | 1/492 [00:02<21:19,  2.61s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 2/492 [00:03<13:45,  1.68s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 3/492 [00:04<11:18,  1.39s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 4/492 [00:05<10:09,  1.25s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 5/492 [00:06<09:30,  1.17s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 6/492 [00:07<09:07,  1.13s/it]\u001b[0m\n",
      "\u001b[34m1%|â–         | 7/492 [00:08<08:52,  1.10s/it]\u001b[0m\n",
      "\u001b[34m2%|â–         | 8/492 [00:09<08:43,  1.08s/it]\u001b[0m\n",
      "\u001b[34m2%|â–         | 9/492 [00:10<08:36,  1.07s/it]\u001b[0m\n",
      "\u001b[34m2%|â–         | 10/492 [00:11<08:30,  1.06s/it]\u001b[0m\n",
      "\u001b[34m2%|â–         | 11/492 [00:12<08:26,  1.05s/it]\u001b[0m\n",
      "\u001b[34m2%|â–         | 12/492 [00:14<08:24,  1.05s/it]\u001b[0m\n",
      "\u001b[34m3%|â–         | 13/492 [00:15<08:20,  1.05s/it]\u001b[0m\n",
      "\u001b[34m3%|â–         | 14/492 [00:16<08:20,  1.05s/it]\u001b[0m\n",
      "\u001b[34m3%|â–         | 15/492 [00:17<08:21,  1.05s/it]\u001b[0m\n",
      "\u001b[34m3%|â–         | 16/492 [00:18<08:18,  1.05s/it]\u001b[0m\n",
      "\u001b[34m3%|â–         | 17/492 [00:19<08:17,  1.05s/it]\u001b[0m\n",
      "\u001b[34m4%|â–         | 18/492 [00:20<08:17,  1.05s/it]\u001b[0m\n",
      "\u001b[34m4%|â–         | 19/492 [00:21<08:16,  1.05s/it]\u001b[0m\n",
      "\u001b[34m4%|â–         | 20/492 [00:22<08:15,  1.05s/it]\u001b[0m\n",
      "\u001b[34m4%|â–         | 21/492 [00:23<08:12,  1.04s/it]\u001b[0m\n",
      "\u001b[34m4%|â–         | 22/492 [00:24<08:12,  1.05s/it]\u001b[0m\n",
      "\u001b[34m5%|â–         | 23/492 [00:25<08:13,  1.05s/it]\u001b[0m\n",
      "\u001b[34m5%|â–         | 24/492 [00:26<08:12,  1.05s/it]\u001b[0m\n",
      "\u001b[34m5%|â–Œ         | 25/492 [00:27<08:12,  1.05s/it]\u001b[0m\n",
      "\u001b[34m5%|â–Œ         | 26/492 [00:28<08:09,  1.05s/it]\u001b[0m\n",
      "\u001b[34m5%|â–Œ         | 27/492 [00:29<08:08,  1.05s/it]\u001b[0m\n",
      "\u001b[34m6%|â–Œ         | 28/492 [00:30<08:07,  1.05s/it]\u001b[0m\n",
      "\u001b[34m6%|â–Œ         | 29/492 [00:31<08:07,  1.05s/it]\u001b[0m\n",
      "\u001b[34m6%|â–Œ         | 30/492 [00:32<08:05,  1.05s/it]\u001b[0m\n",
      "\u001b[34m6%|â–‹         | 31/492 [00:33<08:05,  1.05s/it]\u001b[0m\n",
      "\u001b[34m7%|â–‹         | 32/492 [00:35<08:05,  1.05s/it]\u001b[0m\n",
      "\u001b[34m7%|â–‹         | 33/492 [00:36<08:04,  1.06s/it]\u001b[0m\n",
      "\u001b[34m7%|â–‹         | 34/492 [00:37<08:03,  1.06s/it]\u001b[0m\n",
      "\u001b[34m7%|â–‹         | 35/492 [00:38<08:03,  1.06s/it]\u001b[0m\n",
      "\u001b[34m7%|â–‹         | 36/492 [00:39<08:03,  1.06s/it]\u001b[0m\n",
      "\u001b[34m8%|â–Š         | 37/492 [00:40<08:04,  1.06s/it]\u001b[0m\n",
      "\u001b[34m8%|â–Š         | 38/492 [00:41<08:02,  1.06s/it]\u001b[0m\n",
      "\u001b[34m8%|â–Š         | 39/492 [00:42<08:03,  1.07s/it]\u001b[0m\n",
      "\u001b[34m8%|â–Š         | 40/492 [00:43<08:03,  1.07s/it]\u001b[0m\n",
      "\u001b[34m8%|â–Š         | 41/492 [00:44<08:03,  1.07s/it]\u001b[0m\n",
      "\u001b[34m9%|â–Š         | 42/492 [00:45<08:02,  1.07s/it]\u001b[0m\n",
      "\u001b[34m9%|â–Š         | 43/492 [00:46<08:05,  1.08s/it]\u001b[0m\n",
      "\u001b[34m9%|â–‰         | 44/492 [00:47<08:02,  1.08s/it]\u001b[0m\n",
      "\u001b[34m9%|â–‰         | 45/492 [00:48<07:59,  1.07s/it]\u001b[0m\n",
      "\u001b[34m9%|â–‰         | 46/492 [00:50<07:57,  1.07s/it]\u001b[0m\n",
      "\u001b[34m10%|â–‰         | 47/492 [00:51<07:58,  1.08s/it]\u001b[0m\n",
      "\u001b[34m10%|â–‰         | 48/492 [00:52<07:56,  1.07s/it]\u001b[0m\n",
      "\u001b[34m10%|â–‰         | 49/492 [00:53<07:55,  1.07s/it]\u001b[0m\n",
      "\u001b[34m10%|â–ˆ         | 50/492 [00:54<07:55,  1.08s/it]\u001b[0m\n",
      "\u001b[34m10%|â–ˆ         | 51/492 [00:55<07:54,  1.08s/it]\u001b[0m\n",
      "\u001b[34m11%|â–ˆ         | 52/492 [00:56<07:54,  1.08s/it]\u001b[0m\n",
      "\u001b[34m11%|â–ˆ         | 53/492 [00:57<07:54,  1.08s/it]\u001b[0m\n",
      "\u001b[34m11%|â–ˆ         | 54/492 [00:58<07:54,  1.08s/it]\u001b[0m\n",
      "\u001b[34m11%|â–ˆ         | 55/492 [00:59<07:54,  1.09s/it]\u001b[0m\n",
      "\u001b[34m11%|â–ˆâ–        | 56/492 [01:00<07:50,  1.08s/it]\u001b[0m\n",
      "\u001b[34m12%|â–ˆâ–        | 57/492 [01:01<07:50,  1.08s/it]\u001b[0m\n",
      "\u001b[34m12%|â–ˆâ–        | 58/492 [01:02<07:48,  1.08s/it]\u001b[0m\n",
      "\u001b[34m12%|â–ˆâ–        | 59/492 [01:04<07:47,  1.08s/it]\u001b[0m\n",
      "\u001b[34m12%|â–ˆâ–        | 60/492 [01:05<07:48,  1.08s/it]\u001b[0m\n",
      "\u001b[34m12%|â–ˆâ–        | 61/492 [01:06<07:47,  1.08s/it]\u001b[0m\n",
      "\u001b[34m13%|â–ˆâ–        | 62/492 [01:07<07:47,  1.09s/it]\u001b[0m\n",
      "\u001b[34m13%|â–ˆâ–        | 63/492 [01:08<07:46,  1.09s/it]\u001b[0m\n",
      "\u001b[34m13%|â–ˆâ–        | 64/492 [01:09<07:44,  1.09s/it]\u001b[0m\n",
      "\u001b[34m13%|â–ˆâ–        | 65/492 [01:10<07:43,  1.09s/it]\u001b[0m\n",
      "\u001b[34m13%|â–ˆâ–        | 66/492 [01:11<07:41,  1.08s/it]\u001b[0m\n",
      "\u001b[34m14%|â–ˆâ–        | 67/492 [01:12<07:38,  1.08s/it]\u001b[0m\n",
      "\u001b[34m14%|â–ˆâ–        | 68/492 [01:13<07:38,  1.08s/it]\u001b[0m\n",
      "\u001b[34m14%|â–ˆâ–        | 69/492 [01:14<07:37,  1.08s/it]\u001b[0m\n",
      "\u001b[34m14%|â–ˆâ–        | 70/492 [01:16<07:39,  1.09s/it]\u001b[0m\n",
      "\u001b[34m14%|â–ˆâ–        | 71/492 [01:17<07:37,  1.09s/it]\u001b[0m\n",
      "\u001b[34m15%|â–ˆâ–        | 72/492 [01:18<07:37,  1.09s/it]\u001b[0m\n",
      "\u001b[34m15%|â–ˆâ–        | 73/492 [01:19<07:36,  1.09s/it]\u001b[0m\n",
      "\u001b[34m15%|â–ˆâ–Œ        | 74/492 [01:20<07:35,  1.09s/it]\u001b[0m\n",
      "\u001b[34m15%|â–ˆâ–Œ        | 75/492 [01:21<07:34,  1.09s/it]\u001b[0m\n",
      "\u001b[34m15%|â–ˆâ–Œ        | 76/492 [01:22<07:33,  1.09s/it]\u001b[0m\n",
      "\u001b[34m16%|â–ˆâ–Œ        | 77/492 [01:23<07:33,  1.09s/it]\u001b[0m\n",
      "\u001b[34m16%|â–ˆâ–Œ        | 78/492 [01:24<07:33,  1.10s/it]\u001b[0m\n",
      "\u001b[34m16%|â–ˆâ–Œ        | 79/492 [01:25<07:33,  1.10s/it]\u001b[0m\n",
      "\u001b[34m16%|â–ˆâ–‹        | 80/492 [01:26<07:32,  1.10s/it]\u001b[0m\n",
      "\u001b[34m16%|â–ˆâ–‹        | 81/492 [01:28<07:30,  1.10s/it]\u001b[0m\n",
      "\u001b[34m17%|â–ˆâ–‹        | 82/492 [01:29<07:29,  1.10s/it]\u001b[0m\n",
      "\u001b[34m17%|â–ˆâ–‹        | 83/492 [01:30<07:27,  1.09s/it]\u001b[0m\n",
      "\u001b[34m17%|â–ˆâ–‹        | 84/492 [01:31<07:26,  1.10s/it]\u001b[0m\n",
      "\u001b[34m17%|â–ˆâ–‹        | 85/492 [01:32<07:26,  1.10s/it]\u001b[0m\n",
      "\u001b[34m17%|â–ˆâ–‹        | 86/492 [01:33<07:25,  1.10s/it]\u001b[0m\n",
      "\u001b[34m18%|â–ˆâ–Š        | 87/492 [01:34<07:25,  1.10s/it]\u001b[0m\n",
      "\u001b[34m18%|â–ˆâ–Š        | 88/492 [01:35<07:25,  1.10s/it]\u001b[0m\n",
      "\u001b[34m18%|â–ˆâ–Š        | 89/492 [01:36<07:24,  1.10s/it]\u001b[0m\n",
      "\u001b[34m18%|â–ˆâ–Š        | 90/492 [01:37<07:23,  1.10s/it]\u001b[0m\n",
      "\u001b[34m18%|â–ˆâ–Š        | 91/492 [01:39<07:22,  1.10s/it]\u001b[0m\n",
      "\u001b[34m19%|â–ˆâ–Š        | 92/492 [01:40<07:23,  1.11s/it]\u001b[0m\n",
      "\u001b[34m19%|â–ˆâ–‰        | 93/492 [01:41<07:22,  1.11s/it]\u001b[0m\n",
      "\u001b[34m19%|â–ˆâ–‰        | 94/492 [01:42<07:21,  1.11s/it]\u001b[0m\n",
      "\u001b[34m19%|â–ˆâ–‰        | 95/492 [01:43<07:18,  1.11s/it]\u001b[0m\n",
      "\u001b[34m20%|â–ˆâ–‰        | 96/492 [01:44<07:16,  1.10s/it]\u001b[0m\n",
      "\u001b[34m20%|â–ˆâ–‰        | 97/492 [01:45<07:14,  1.10s/it]\u001b[0m\n",
      "\u001b[34m20%|â–ˆâ–‰        | 98/492 [01:46<07:13,  1.10s/it]\u001b[0m\n",
      "\u001b[34m20%|â–ˆâ–ˆ        | 99/492 [01:47<07:11,  1.10s/it]\u001b[0m\n",
      "\u001b[34m20%|â–ˆâ–ˆ        | 100/492 [01:48<07:10,  1.10s/it]\u001b[0m\n",
      "\u001b[34m21%|â–ˆâ–ˆ        | 101/492 [01:50<07:09,  1.10s/it]\u001b[0m\n",
      "\u001b[34m21%|â–ˆâ–ˆ        | 102/492 [01:51<07:07,  1.10s/it]\u001b[0m\n",
      "\u001b[34m21%|â–ˆâ–ˆ        | 103/492 [01:52<07:06,  1.10s/it]\u001b[0m\n",
      "\u001b[34m21%|â–ˆâ–ˆ        | 104/492 [01:53<07:04,  1.09s/it]\u001b[0m\n",
      "\u001b[34m21%|â–ˆâ–ˆâ–       | 105/492 [01:54<07:04,  1.10s/it]\u001b[0m\n",
      "\u001b[34m22%|â–ˆâ–ˆâ–       | 106/492 [01:55<07:03,  1.10s/it]\u001b[0m\n",
      "\u001b[34m22%|â–ˆâ–ˆâ–       | 107/492 [01:56<07:04,  1.10s/it]\u001b[0m\n",
      "\u001b[34m22%|â–ˆâ–ˆâ–       | 108/492 [01:57<07:03,  1.10s/it]\u001b[0m\n",
      "\u001b[34m22%|â–ˆâ–ˆâ–       | 109/492 [01:58<07:01,  1.10s/it]\u001b[0m\n",
      "\u001b[34m22%|â–ˆâ–ˆâ–       | 110/492 [01:59<07:01,  1.10s/it]\u001b[0m\n",
      "\u001b[34m23%|â–ˆâ–ˆâ–       | 111/492 [02:01<07:00,  1.10s/it]\u001b[0m\n",
      "\u001b[34m23%|â–ˆâ–ˆâ–       | 112/492 [02:02<06:59,  1.10s/it]\u001b[0m\n",
      "\u001b[34m23%|â–ˆâ–ˆâ–       | 113/492 [02:03<06:59,  1.11s/it]\u001b[0m\n",
      "\u001b[34m23%|â–ˆâ–ˆâ–       | 114/492 [02:04<06:57,  1.11s/it]\u001b[0m\n",
      "\u001b[34m23%|â–ˆâ–ˆâ–       | 115/492 [02:05<06:56,  1.10s/it]\u001b[0m\n",
      "\u001b[34m24%|â–ˆâ–ˆâ–       | 116/492 [02:06<06:55,  1.10s/it]\u001b[0m\n",
      "\u001b[34m24%|â–ˆâ–ˆâ–       | 117/492 [02:07<06:54,  1.10s/it]\u001b[0m\n",
      "\u001b[34m24%|â–ˆâ–ˆâ–       | 118/492 [02:08<06:52,  1.10s/it]\u001b[0m\n",
      "\u001b[34m24%|â–ˆâ–ˆâ–       | 119/492 [02:09<06:53,  1.11s/it]\u001b[0m\n",
      "\u001b[34m24%|â–ˆâ–ˆâ–       | 120/492 [02:11<06:51,  1.11s/it]\u001b[0m\n",
      "\u001b[34m25%|â–ˆâ–ˆâ–       | 121/492 [02:12<06:50,  1.11s/it]\u001b[0m\n",
      "\u001b[34m25%|â–ˆâ–ˆâ–       | 122/492 [02:13<06:49,  1.11s/it]\u001b[0m\n",
      "\u001b[34m25%|â–ˆâ–ˆâ–Œ       | 123/492 [02:14<06:47,  1.11s/it]\u001b[0m\n",
      "\u001b[34m25%|â–ˆâ–ˆâ–Œ       | 124/492 [02:15<06:47,  1.11s/it]\u001b[0m\n",
      "\u001b[34m25%|â–ˆâ–ˆâ–Œ       | 125/492 [02:16<06:47,  1.11s/it]\u001b[0m\n",
      "\u001b[34m26%|â–ˆâ–ˆâ–Œ       | 126/492 [02:17<06:45,  1.11s/it]\u001b[0m\n",
      "\u001b[34m26%|â–ˆâ–ˆâ–Œ       | 127/492 [02:18<06:44,  1.11s/it]\u001b[0m\n",
      "\u001b[34m26%|â–ˆâ–ˆâ–Œ       | 128/492 [02:19<06:44,  1.11s/it]\u001b[0m\n",
      "\u001b[34m26%|â–ˆâ–ˆâ–Œ       | 129/492 [02:21<06:44,  1.12s/it]\u001b[0m\n",
      "\u001b[34m26%|â–ˆâ–ˆâ–‹       | 130/492 [02:22<06:44,  1.12s/it]\u001b[0m\n",
      "\u001b[34m27%|â–ˆâ–ˆâ–‹       | 131/492 [02:23<06:43,  1.12s/it]\u001b[0m\n",
      "\u001b[34m27%|â–ˆâ–ˆâ–‹       | 132/492 [02:24<06:43,  1.12s/it]\u001b[0m\n",
      "\u001b[34m27%|â–ˆâ–ˆâ–‹       | 133/492 [02:25<06:42,  1.12s/it]\u001b[0m\n",
      "\u001b[34m27%|â–ˆâ–ˆâ–‹       | 134/492 [02:26<06:41,  1.12s/it]\u001b[0m\n",
      "\u001b[34m27%|â–ˆâ–ˆâ–‹       | 135/492 [02:27<06:40,  1.12s/it]\u001b[0m\n",
      "\u001b[34m28%|â–ˆâ–ˆâ–Š       | 136/492 [02:28<06:40,  1.13s/it]\u001b[0m\n",
      "\u001b[34m28%|â–ˆâ–ˆâ–Š       | 137/492 [02:30<06:38,  1.12s/it]\u001b[0m\n",
      "\u001b[34m28%|â–ˆâ–ˆâ–Š       | 138/492 [02:31<06:36,  1.12s/it]\u001b[0m\n",
      "\u001b[34m28%|â–ˆâ–ˆâ–Š       | 139/492 [02:32<06:35,  1.12s/it]\u001b[0m\n",
      "\u001b[34m28%|â–ˆâ–ˆâ–Š       | 140/492 [02:33<06:35,  1.12s/it]\u001b[0m\n",
      "\u001b[34m29%|â–ˆâ–ˆâ–Š       | 141/492 [02:34<06:35,  1.13s/it]\u001b[0m\n",
      "\u001b[34m29%|â–ˆâ–ˆâ–‰       | 142/492 [02:35<06:32,  1.12s/it]\u001b[0m\n",
      "\u001b[34m29%|â–ˆâ–ˆâ–‰       | 143/492 [02:36<06:31,  1.12s/it]\u001b[0m\n",
      "\u001b[34m29%|â–ˆâ–ˆâ–‰       | 144/492 [02:37<06:30,  1.12s/it]\u001b[0m\n",
      "\u001b[34m29%|â–ˆâ–ˆâ–‰       | 145/492 [02:38<06:29,  1.12s/it]\u001b[0m\n",
      "\u001b[34m30%|â–ˆâ–ˆâ–‰       | 146/492 [02:40<06:29,  1.12s/it]\u001b[0m\n",
      "\u001b[34m30%|â–ˆâ–ˆâ–‰       | 147/492 [02:41<06:28,  1.13s/it]\u001b[0m\n",
      "\u001b[34m30%|â–ˆâ–ˆâ–ˆ       | 148/492 [02:42<06:26,  1.12s/it]\u001b[0m\n",
      "\u001b[34m30%|â–ˆâ–ˆâ–ˆ       | 149/492 [02:43<06:24,  1.12s/it]\u001b[0m\n",
      "\u001b[34m30%|â–ˆâ–ˆâ–ˆ       | 150/492 [02:44<06:24,  1.12s/it]\u001b[0m\n",
      "\u001b[34m31%|â–ˆâ–ˆâ–ˆ       | 151/492 [02:45<06:24,  1.13s/it]\u001b[0m\n",
      "\u001b[34m31%|â–ˆâ–ˆâ–ˆ       | 152/492 [02:46<06:23,  1.13s/it]\u001b[0m\n",
      "\u001b[34m31%|â–ˆâ–ˆâ–ˆ       | 153/492 [02:48<06:22,  1.13s/it]\u001b[0m\n",
      "\u001b[34m31%|â–ˆâ–ˆâ–ˆâ–      | 154/492 [02:49<06:21,  1.13s/it]\u001b[0m\n",
      "\u001b[34m32%|â–ˆâ–ˆâ–ˆâ–      | 155/492 [02:50<06:21,  1.13s/it]\u001b[0m\n",
      "\u001b[34m32%|â–ˆâ–ˆâ–ˆâ–      | 156/492 [02:51<06:20,  1.13s/it]\u001b[0m\n",
      "\u001b[34m32%|â–ˆâ–ˆâ–ˆâ–      | 157/492 [02:52<06:19,  1.13s/it]\u001b[0m\n",
      "\u001b[34m32%|â–ˆâ–ˆâ–ˆâ–      | 158/492 [02:53<06:18,  1.13s/it]\u001b[0m\n",
      "\u001b[34m32%|â–ˆâ–ˆâ–ˆâ–      | 159/492 [02:54<06:19,  1.14s/it]\u001b[0m\n",
      "\u001b[34m33%|â–ˆâ–ˆâ–ˆâ–      | 160/492 [02:55<06:18,  1.14s/it]\u001b[0m\n",
      "\u001b[34m33%|â–ˆâ–ˆâ–ˆâ–      | 161/492 [02:57<06:17,  1.14s/it]\u001b[0m\n",
      "\u001b[34m33%|â–ˆâ–ˆâ–ˆâ–      | 162/492 [02:58<06:15,  1.14s/it]\u001b[0m\n",
      "\u001b[34m33%|â–ˆâ–ˆâ–ˆâ–      | 163/492 [02:59<06:15,  1.14s/it]\u001b[0m\n",
      "\u001b[34m33%|â–ˆâ–ˆâ–ˆâ–      | 164/492 [03:00<06:14,  1.14s/it]\u001b[0m\n",
      "\u001b[34m34%|â–ˆâ–ˆâ–ˆâ–      | 165/492 [03:01<06:13,  1.14s/it]\u001b[0m\n",
      "\u001b[34m34%|â–ˆâ–ˆâ–ˆâ–      | 166/492 [03:02<06:12,  1.14s/it]\u001b[0m\n",
      "\u001b[34m34%|â–ˆâ–ˆâ–ˆâ–      | 167/492 [03:03<06:10,  1.14s/it]\u001b[0m\n",
      "\u001b[34m34%|â–ˆâ–ˆâ–ˆâ–      | 168/492 [03:05<06:09,  1.14s/it]\u001b[0m\n",
      "\u001b[34m34%|â–ˆâ–ˆâ–ˆâ–      | 169/492 [03:06<06:10,  1.15s/it]\u001b[0m\n",
      "\u001b[34m35%|â–ˆâ–ˆâ–ˆâ–      | 170/492 [03:07<06:09,  1.15s/it]\u001b[0m\n",
      "\u001b[34m35%|â–ˆâ–ˆâ–ˆâ–      | 171/492 [03:08<06:07,  1.14s/it]\u001b[0m\n",
      "\u001b[34m35%|â–ˆâ–ˆâ–ˆâ–      | 172/492 [03:09<06:07,  1.15s/it]\u001b[0m\n",
      "\u001b[34m35%|â–ˆâ–ˆâ–ˆâ–Œ      | 173/492 [03:10<06:05,  1.15s/it]\u001b[0m\n",
      "\u001b[34m35%|â–ˆâ–ˆâ–ˆâ–Œ      | 174/492 [03:11<06:04,  1.15s/it]\u001b[0m\n",
      "\u001b[34m36%|â–ˆâ–ˆâ–ˆâ–Œ      | 175/492 [03:13<06:02,  1.14s/it]\u001b[0m\n",
      "\u001b[34m36%|â–ˆâ–ˆâ–ˆâ–Œ      | 176/492 [03:14<06:01,  1.14s/it]\u001b[0m\n",
      "\u001b[34m36%|â–ˆâ–ˆâ–ˆâ–Œ      | 177/492 [03:15<06:02,  1.15s/it]\u001b[0m\n",
      "\u001b[34m36%|â–ˆâ–ˆâ–ˆâ–Œ      | 178/492 [03:16<06:00,  1.15s/it]\u001b[0m\n",
      "\u001b[34m36%|â–ˆâ–ˆâ–ˆâ–‹      | 179/492 [03:17<05:59,  1.15s/it]\u001b[0m\n",
      "\u001b[34m37%|â–ˆâ–ˆâ–ˆâ–‹      | 180/492 [03:18<05:58,  1.15s/it]\u001b[0m\n",
      "\u001b[34m37%|â–ˆâ–ˆâ–ˆâ–‹      | 181/492 [03:20<05:57,  1.15s/it]\u001b[0m\n",
      "\u001b[34m37%|â–ˆâ–ˆâ–ˆâ–‹      | 182/492 [03:21<05:55,  1.15s/it]\u001b[0m\n",
      "\u001b[34m37%|â–ˆâ–ˆâ–ˆâ–‹      | 183/492 [03:22<05:54,  1.15s/it]\u001b[0m\n",
      "\u001b[34m37%|â–ˆâ–ˆâ–ˆâ–‹      | 184/492 [03:23<05:54,  1.15s/it]\u001b[0m\n",
      "\u001b[34m38%|â–ˆâ–ˆâ–ˆâ–Š      | 185/492 [03:24<05:53,  1.15s/it]\u001b[0m\n",
      "\u001b[34m38%|â–ˆâ–ˆâ–ˆâ–Š      | 186/492 [03:25<05:51,  1.15s/it]\u001b[0m\n",
      "\u001b[34m38%|â–ˆâ–ˆâ–ˆâ–Š      | 187/492 [03:26<05:51,  1.15s/it]\u001b[0m\n",
      "\u001b[34m38%|â–ˆâ–ˆâ–ˆâ–Š      | 188/492 [03:28<05:50,  1.15s/it]\u001b[0m\n",
      "\u001b[34m38%|â–ˆâ–ˆâ–ˆâ–Š      | 189/492 [03:29<05:49,  1.15s/it]\u001b[0m\n",
      "\u001b[34m39%|â–ˆâ–ˆâ–ˆâ–Š      | 190/492 [03:30<05:47,  1.15s/it]\u001b[0m\n",
      "\u001b[34m39%|â–ˆâ–ˆâ–ˆâ–‰      | 191/492 [03:31<05:47,  1.15s/it]\u001b[0m\n",
      "\u001b[34m39%|â–ˆâ–ˆâ–ˆâ–‰      | 192/492 [03:32<05:46,  1.15s/it]\u001b[0m\n",
      "\u001b[34m39%|â–ˆâ–ˆâ–ˆâ–‰      | 193/492 [03:33<05:44,  1.15s/it]\u001b[0m\n",
      "\u001b[34m39%|â–ˆâ–ˆâ–ˆâ–‰      | 194/492 [03:35<05:44,  1.16s/it]\u001b[0m\n",
      "\u001b[34m40%|â–ˆâ–ˆâ–ˆâ–‰      | 195/492 [03:36<05:42,  1.15s/it]\u001b[0m\n",
      "\u001b[34m40%|â–ˆâ–ˆâ–ˆâ–‰      | 196/492 [03:37<05:42,  1.16s/it]\u001b[0m\n",
      "\u001b[34m40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 197/492 [03:38<05:41,  1.16s/it]\u001b[0m\n",
      "\u001b[34m40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 198/492 [03:39<05:40,  1.16s/it]\u001b[0m\n",
      "\u001b[34m40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 199/492 [03:40<05:40,  1.16s/it]\u001b[0m\n",
      "\u001b[34m41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 200/492 [03:41<05:39,  1.16s/it]\u001b[0m\n",
      "\u001b[34m41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 201/492 [03:43<05:37,  1.16s/it]\u001b[0m\n",
      "\u001b[34m41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 202/492 [03:44<05:36,  1.16s/it]\u001b[0m\n",
      "\u001b[34m41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 203/492 [03:45<05:35,  1.16s/it]\u001b[0m\n",
      "\u001b[34m41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 204/492 [03:46<05:35,  1.17s/it]\u001b[0m\n",
      "\u001b[34m42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 205/492 [03:47<05:33,  1.16s/it]\u001b[0m\n",
      "\u001b[34m42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 206/492 [03:48<05:31,  1.16s/it]\u001b[0m\n",
      "\u001b[34m42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 207/492 [03:50<05:31,  1.16s/it]\u001b[0m\n",
      "\u001b[34m42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 208/492 [03:51<05:30,  1.16s/it]\u001b[0m\n",
      "\u001b[34m42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 209/492 [03:52<05:29,  1.16s/it]\u001b[0m\n",
      "\u001b[34m43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 210/492 [03:53<05:28,  1.17s/it]\u001b[0m\n",
      "\u001b[34m43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 211/492 [03:54<05:26,  1.16s/it]\u001b[0m\n",
      "\u001b[34m43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 212/492 [03:55<05:26,  1.17s/it]\u001b[0m\n",
      "\u001b[34m43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 213/492 [03:57<05:24,  1.16s/it]\u001b[0m\n",
      "\u001b[34m43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 214/492 [03:58<05:24,  1.17s/it]\u001b[0m\n",
      "\u001b[34m44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 215/492 [03:59<05:22,  1.16s/it]\u001b[0m\n",
      "\u001b[34m44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 216/492 [04:00<05:21,  1.16s/it]\u001b[0m\n",
      "\u001b[34m44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 217/492 [04:01<05:21,  1.17s/it]\u001b[0m\n",
      "\u001b[34m44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 218/492 [04:02<05:21,  1.17s/it]\u001b[0m\n",
      "\u001b[34m45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 219/492 [04:04<05:20,  1.17s/it]\u001b[0m\n",
      "\u001b[34m45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 220/492 [04:05<05:19,  1.17s/it]\u001b[0m\n",
      "\u001b[34m45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 221/492 [04:06<05:17,  1.17s/it]\u001b[0m\n",
      "\u001b[34m45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 222/492 [04:07<05:16,  1.17s/it]\u001b[0m\n",
      "\u001b[34m45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 223/492 [04:08<05:14,  1.17s/it]\u001b[0m\n",
      "\u001b[34m46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 224/492 [04:09<05:11,  1.16s/it]\u001b[0m\n",
      "\u001b[34m46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 225/492 [04:11<05:08,  1.16s/it]\u001b[0m\n",
      "\u001b[34m46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 226/492 [04:12<05:06,  1.15s/it]\u001b[0m\n",
      "\u001b[34m46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 227/492 [04:13<05:06,  1.15s/it]\u001b[0m\n",
      "\u001b[34m46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 228/492 [04:14<05:04,  1.15s/it]\u001b[0m\n",
      "\u001b[34m47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 229/492 [04:15<05:03,  1.15s/it]\u001b[0m\n",
      "\u001b[34m47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 230/492 [04:16<05:02,  1.15s/it]\u001b[0m\n",
      "\u001b[34m47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 231/492 [04:18<05:00,  1.15s/it]\u001b[0m\n",
      "\u001b[34m47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 232/492 [04:19<05:00,  1.16s/it]\u001b[0m\n",
      "\u001b[34m47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 233/492 [04:20<04:58,  1.15s/it]\u001b[0m\n",
      "\u001b[34m48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 234/492 [04:21<04:57,  1.15s/it]\u001b[0m\n",
      "\u001b[34m48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 235/492 [04:22<04:56,  1.16s/it]\u001b[0m\n",
      "\u001b[34m48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 236/492 [04:23<04:55,  1.16s/it]\u001b[0m\n",
      "\u001b[34m48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 237/492 [04:24<04:54,  1.16s/it]\u001b[0m\n",
      "\u001b[34m48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 238/492 [04:26<04:54,  1.16s/it]\u001b[0m\n",
      "\u001b[34m49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 239/492 [04:27<04:52,  1.16s/it]\u001b[0m\n",
      "\u001b[34m49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 240/492 [04:28<04:51,  1.15s/it]\u001b[0m\n",
      "\u001b[34m49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 241/492 [04:29<04:50,  1.16s/it]\u001b[0m\n",
      "\u001b[34m49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 242/492 [04:30<04:48,  1.15s/it]\u001b[0m\n",
      "\u001b[34m49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 243/492 [04:31<04:47,  1.15s/it]\u001b[0m\n",
      "\u001b[34m50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 244/492 [04:33<04:46,  1.16s/it]\u001b[0m\n",
      "\u001b[34m50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 245/492 [04:34<04:45,  1.16s/it]\u001b[0m\n",
      "\u001b[34m50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 246/492 [04:35<04:44,  1.16s/it]\u001b[0m\n",
      "\u001b[34m50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 247/492 [04:36<04:43,  1.16s/it]\u001b[0m\n",
      "\u001b[34m50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 248/492 [04:37<04:43,  1.16s/it]\u001b[0m\n",
      "\u001b[34m51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 249/492 [04:38<04:42,  1.16s/it]\u001b[0m\n",
      "\u001b[34m51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 250/492 [04:40<04:41,  1.16s/it]\u001b[0m\n",
      "\u001b[34m51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 251/492 [04:41<04:39,  1.16s/it]\u001b[0m\n",
      "\u001b[34m51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 252/492 [04:42<04:37,  1.16s/it]\u001b[0m\n",
      "\u001b[34m51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 253/492 [04:43<04:36,  1.16s/it]\u001b[0m\n",
      "\u001b[34m52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 254/492 [04:44<04:35,  1.16s/it]\u001b[0m\n",
      "\u001b[34m52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 255/492 [04:45<04:34,  1.16s/it]\u001b[0m\n",
      "\u001b[34m52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 256/492 [04:46<04:32,  1.16s/it]\u001b[0m\n",
      "\u001b[34m52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 257/492 [04:48<04:31,  1.15s/it]\u001b[0m\n",
      "\u001b[34m52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 258/492 [04:49<04:28,  1.15s/it]\u001b[0m\n",
      "\u001b[34m53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 259/492 [04:50<04:28,  1.15s/it]\u001b[0m\n",
      "\u001b[34m53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 260/492 [04:51<04:27,  1.15s/it]\u001b[0m\n",
      "\u001b[34m53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 261/492 [04:52<04:27,  1.16s/it]\u001b[0m\n",
      "\u001b[34m53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 262/492 [04:53<04:26,  1.16s/it]\u001b[0m\n",
      "\u001b[34m53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 263/492 [04:55<04:25,  1.16s/it]\u001b[0m\n",
      "\u001b[34m54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 264/492 [04:56<04:24,  1.16s/it]\u001b[0m\n",
      "\u001b[34m54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 265/492 [04:57<04:23,  1.16s/it]\u001b[0m\n",
      "\u001b[34m54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 266/492 [04:58<04:22,  1.16s/it]\u001b[0m\n",
      "\u001b[34m54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 267/492 [04:59<04:22,  1.16s/it]\u001b[0m\n",
      "\u001b[34m54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 268/492 [05:00<04:21,  1.17s/it]\u001b[0m\n",
      "\u001b[34m55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 269/492 [05:02<04:19,  1.17s/it]\u001b[0m\n",
      "\u001b[34m55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 270/492 [05:03<04:18,  1.17s/it]\u001b[0m\n",
      "\u001b[34m55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 271/492 [05:04<04:17,  1.16s/it]\u001b[0m\n",
      "\u001b[34m55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 272/492 [05:05<04:15,  1.16s/it]\u001b[0m\n",
      "\u001b[34m55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 273/492 [05:06<04:14,  1.16s/it]\u001b[0m\n",
      "\u001b[34m56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 274/492 [05:07<04:14,  1.17s/it]\u001b[0m\n",
      "\u001b[34m56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 275/492 [05:09<04:13,  1.17s/it]\u001b[0m\n",
      "\u001b[34m56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 276/492 [05:10<04:12,  1.17s/it]\u001b[0m\n",
      "\u001b[34m56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 277/492 [05:11<04:12,  1.17s/it]\u001b[0m\n",
      "\u001b[34m57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 278/492 [05:12<04:10,  1.17s/it]\u001b[0m\n",
      "\u001b[34m57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 279/492 [05:13<04:08,  1.17s/it]\u001b[0m\n",
      "\u001b[34m57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 280/492 [05:14<04:07,  1.17s/it]\u001b[0m\n",
      "\u001b[34m57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 281/492 [05:16<04:05,  1.16s/it]\u001b[0m\n",
      "\u001b[34m57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 282/492 [05:17<04:04,  1.17s/it]\u001b[0m\n",
      "\u001b[34m58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 283/492 [05:18<04:03,  1.17s/it]\u001b[0m\n",
      "\u001b[34m58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 284/492 [05:19<04:02,  1.17s/it]\u001b[0m\n",
      "\u001b[34m58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 285/492 [05:20<04:01,  1.17s/it]\u001b[0m\n",
      "\u001b[34m58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 286/492 [05:21<04:00,  1.17s/it]\u001b[0m\n",
      "\u001b[34m58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 287/492 [05:23<03:59,  1.17s/it]\u001b[0m\n",
      "\u001b[34m59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 288/492 [05:24<03:57,  1.17s/it]\u001b[0m\n",
      "\u001b[34m59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 289/492 [05:25<03:57,  1.17s/it]\u001b[0m\n",
      "\u001b[34m59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 290/492 [05:26<03:55,  1.17s/it]\u001b[0m\n",
      "\u001b[34m59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 291/492 [05:27<03:53,  1.16s/it]\u001b[0m\n",
      "\u001b[34m59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 292/492 [05:28<03:52,  1.16s/it]\u001b[0m\n",
      "\u001b[34m60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 293/492 [05:30<03:51,  1.16s/it]\u001b[0m\n",
      "\u001b[34m60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 294/492 [05:31<03:50,  1.16s/it]\u001b[0m\n",
      "\u001b[34m60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 295/492 [05:32<03:49,  1.17s/it]\u001b[0m\n",
      "\u001b[34m60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 296/492 [05:33<03:48,  1.17s/it]\u001b[0m\n",
      "\u001b[34m60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 297/492 [05:34<03:47,  1.17s/it]\u001b[0m\n",
      "\u001b[34m61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 298/492 [05:35<03:45,  1.16s/it]\u001b[0m\n",
      "\u001b[34m61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 299/492 [05:37<03:45,  1.17s/it]\u001b[0m\n",
      "\u001b[34m61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 300/492 [05:38<03:44,  1.17s/it]\u001b[0m\n",
      "\u001b[34m61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 301/492 [05:39<03:42,  1.17s/it]\u001b[0m\n",
      "\u001b[34m61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 302/492 [05:40<03:41,  1.17s/it]\u001b[0m\n",
      "\u001b[34m62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 303/492 [05:41<03:40,  1.17s/it]\u001b[0m\n",
      "\u001b[34m62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 304/492 [05:42<03:38,  1.16s/it]\u001b[0m\n",
      "\u001b[34m62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 305/492 [05:44<03:38,  1.17s/it]\u001b[0m\n",
      "\u001b[34m62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 306/492 [05:45<03:37,  1.17s/it]\u001b[0m\n",
      "\u001b[34m62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 307/492 [05:46<03:37,  1.17s/it]\u001b[0m\n",
      "\u001b[34m63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 308/492 [05:47<03:36,  1.17s/it]\u001b[0m\n",
      "\u001b[34m63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 309/492 [05:48<03:34,  1.17s/it]\u001b[0m\n",
      "\u001b[34m63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 310/492 [05:49<03:33,  1.18s/it]\u001b[0m\n",
      "\u001b[34m63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 311/492 [05:51<03:32,  1.17s/it]\u001b[0m\n",
      "\u001b[34m63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 312/492 [05:52<03:30,  1.17s/it]\u001b[0m\n",
      "\u001b[34m64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 313/492 [05:53<03:29,  1.17s/it]\u001b[0m\n",
      "\u001b[34m64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 314/492 [05:54<03:28,  1.17s/it]\u001b[0m\n",
      "\u001b[34m64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 315/492 [05:55<03:27,  1.17s/it]\u001b[0m\n",
      "\u001b[34m64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 316/492 [05:56<03:25,  1.17s/it]\u001b[0m\n",
      "\u001b[34m64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 317/492 [05:58<03:24,  1.17s/it]\u001b[0m\n",
      "\u001b[34m65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 318/492 [05:59<03:24,  1.17s/it]\u001b[0m\n",
      "\u001b[34m65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 319/492 [06:00<03:22,  1.17s/it]\u001b[0m\n",
      "\u001b[34m65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 320/492 [06:01<03:22,  1.17s/it]\u001b[0m\n",
      "\u001b[34m65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 321/492 [06:02<03:20,  1.17s/it]\u001b[0m\n",
      "\u001b[34m65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 322/492 [06:03<03:18,  1.17s/it]\u001b[0m\n",
      "\u001b[34m66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 323/492 [06:05<03:17,  1.17s/it]\u001b[0m\n",
      "\u001b[34m66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 324/492 [06:06<03:16,  1.17s/it]\u001b[0m\n",
      "\u001b[34m66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 325/492 [06:07<03:16,  1.17s/it]\u001b[0m\n",
      "\u001b[34m66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 326/492 [06:08<03:14,  1.17s/it]\u001b[0m\n",
      "\u001b[34m66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 327/492 [06:09<03:14,  1.18s/it]\u001b[0m\n",
      "\u001b[34m67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 328/492 [06:11<03:12,  1.18s/it]\u001b[0m\n",
      "\u001b[34m67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 329/492 [06:12<03:11,  1.18s/it]\u001b[0m\n",
      "\u001b[34m67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 330/492 [06:13<03:10,  1.17s/it]\u001b[0m\n",
      "\u001b[34m67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 331/492 [06:14<03:09,  1.17s/it]\u001b[0m\n",
      "\u001b[34m67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 332/492 [06:15<03:08,  1.18s/it]\u001b[0m\n",
      "\u001b[34m68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 333/492 [06:16<03:06,  1.17s/it]\u001b[0m\n",
      "\u001b[34m68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 334/492 [06:18<03:06,  1.18s/it]\u001b[0m\n",
      "\u001b[34m68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 335/492 [06:19<03:05,  1.18s/it]\u001b[0m\n",
      "\u001b[34m68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 336/492 [06:20<03:03,  1.18s/it]\u001b[0m\n",
      "\u001b[34m68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 337/492 [06:21<03:02,  1.18s/it]\u001b[0m\n",
      "\u001b[34m69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 338/492 [06:22<03:00,  1.17s/it]\u001b[0m\n",
      "\u001b[34m69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 339/492 [06:23<02:59,  1.17s/it]\u001b[0m\n",
      "\u001b[34m69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 340/492 [06:25<02:58,  1.17s/it]\u001b[0m\n",
      "\u001b[34m69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 341/492 [06:26<02:57,  1.17s/it]\u001b[0m\n",
      "\u001b[34m70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 342/492 [06:27<02:55,  1.17s/it]\u001b[0m\n",
      "\u001b[34m70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 343/492 [06:28<02:54,  1.17s/it]\u001b[0m\n",
      "\u001b[34m70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 344/492 [06:29<02:53,  1.17s/it]\u001b[0m\n",
      "\u001b[34m70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 345/492 [06:30<02:52,  1.17s/it]\u001b[0m\n",
      "\u001b[34m70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 346/492 [06:32<02:51,  1.18s/it]\u001b[0m\n",
      "\u001b[34m71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 347/492 [06:33<02:49,  1.17s/it]\u001b[0m\n",
      "\u001b[34m71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 348/492 [06:34<02:48,  1.17s/it]\u001b[0m\n",
      "\u001b[34m71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 349/492 [06:35<02:47,  1.17s/it]\u001b[0m\n",
      "\u001b[34m71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 350/492 [06:36<02:46,  1.17s/it]\u001b[0m\n",
      "\u001b[34m71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 351/492 [06:37<02:45,  1.17s/it]\u001b[0m\n",
      "\u001b[34m72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 352/492 [06:39<02:43,  1.17s/it]\u001b[0m\n",
      "\u001b[34m72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 353/492 [06:40<02:43,  1.17s/it]\u001b[0m\n",
      "\u001b[34m72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 354/492 [06:41<02:41,  1.17s/it]\u001b[0m\n",
      "\u001b[34m72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 355/492 [06:42<02:40,  1.17s/it]\u001b[0m\n",
      "\u001b[34m72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 356/492 [06:43<02:39,  1.17s/it]\u001b[0m\n",
      "\u001b[34m73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 357/492 [06:45<02:38,  1.18s/it]\u001b[0m\n",
      "\u001b[34m73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 358/492 [06:46<02:38,  1.18s/it]\u001b[0m\n",
      "\u001b[34m73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 359/492 [06:47<02:37,  1.19s/it]\u001b[0m\n",
      "\u001b[34m73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 360/492 [06:48<02:36,  1.18s/it]\u001b[0m\n",
      "\u001b[34m73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 361/492 [06:49<02:34,  1.18s/it]\u001b[0m\n",
      "\u001b[34m74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 362/492 [06:50<02:33,  1.18s/it]\u001b[0m\n",
      "\u001b[34m74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 363/492 [06:52<02:31,  1.18s/it]\u001b[0m\n",
      "\u001b[34m74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 364/492 [06:53<02:31,  1.18s/it]\u001b[0m\n",
      "\u001b[34m74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 365/492 [06:54<02:30,  1.18s/it]\u001b[0m\n",
      "\u001b[34m74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 366/492 [06:55<02:29,  1.18s/it]\u001b[0m\n",
      "\u001b[34m75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 367/492 [06:56<02:27,  1.18s/it]\u001b[0m\n",
      "\u001b[34m75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 368/492 [06:58<02:26,  1.18s/it]\u001b[0m\n",
      "\u001b[34m75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 369/492 [06:59<02:25,  1.18s/it]\u001b[0m\n",
      "\u001b[34m75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 370/492 [07:00<02:24,  1.19s/it]\u001b[0m\n",
      "\u001b[34m75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 371/492 [07:01<02:23,  1.18s/it]\u001b[0m\n",
      "\u001b[34m76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 372/492 [07:02<02:21,  1.18s/it]\u001b[0m\n",
      "\u001b[34m76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 373/492 [07:03<02:20,  1.18s/it]\u001b[0m\n",
      "\u001b[34m76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 374/492 [07:05<02:19,  1.18s/it]\u001b[0m\n",
      "\u001b[34m76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 375/492 [07:06<02:18,  1.19s/it]\u001b[0m\n",
      "\u001b[34m76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 376/492 [07:07<02:16,  1.18s/it]\u001b[0m\n",
      "\u001b[34m77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 377/492 [07:08<02:15,  1.18s/it]\u001b[0m\n",
      "\u001b[34m77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 378/492 [07:09<02:14,  1.18s/it]\u001b[0m\n",
      "\u001b[34m77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 379/492 [07:11<02:13,  1.19s/it]\u001b[0m\n",
      "\u001b[34m77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 380/492 [07:12<02:12,  1.19s/it]\u001b[0m\n",
      "\u001b[34m77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 381/492 [07:13<02:10,  1.18s/it]\u001b[0m\n",
      "\u001b[34m78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 382/492 [07:14<02:09,  1.17s/it]\u001b[0m\n",
      "\u001b[34m78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 383/492 [07:15<02:08,  1.18s/it]\u001b[0m\n",
      "\u001b[34m78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 384/492 [07:16<02:07,  1.18s/it]\u001b[0m\n",
      "\u001b[34m78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 385/492 [07:18<02:06,  1.18s/it]\u001b[0m\n",
      "\u001b[34m78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 386/492 [07:19<02:05,  1.18s/it]\u001b[0m\n",
      "\u001b[34m79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 387/492 [07:20<02:03,  1.18s/it]\u001b[0m\n",
      "\u001b[34m79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 388/492 [07:21<02:02,  1.18s/it]\u001b[0m\n",
      "\u001b[34m79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 389/492 [07:22<02:01,  1.18s/it]\u001b[0m\n",
      "\u001b[34m79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 390/492 [07:24<02:00,  1.18s/it]\u001b[0m\n",
      "\u001b[34m79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 391/492 [07:25<01:59,  1.18s/it]\u001b[0m\n",
      "\u001b[34m80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 392/492 [07:26<01:58,  1.18s/it]\u001b[0m\n",
      "\u001b[34m80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 393/492 [07:27<01:57,  1.18s/it]\u001b[0m\n",
      "\u001b[34m80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 394/492 [07:28<01:55,  1.18s/it]\u001b[0m\n",
      "\u001b[34m80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 395/492 [07:29<01:54,  1.18s/it]\u001b[0m\n",
      "\u001b[34m80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 396/492 [07:31<01:53,  1.18s/it]\u001b[0m\n",
      "\u001b[34m81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 397/492 [07:32<01:52,  1.18s/it]\u001b[0m\n",
      "\u001b[34m81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 398/492 [07:33<01:50,  1.18s/it]\u001b[0m\n",
      "\u001b[34m81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 399/492 [07:34<01:49,  1.18s/it]\u001b[0m\n",
      "\u001b[34m81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 400/492 [07:35<01:48,  1.18s/it]\u001b[0m\n",
      "\u001b[34m82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 401/492 [07:37<01:47,  1.18s/it]\u001b[0m\n",
      "\u001b[34m82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 402/492 [07:38<01:45,  1.18s/it]\u001b[0m\n",
      "\u001b[34m82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 403/492 [07:39<01:45,  1.18s/it]\u001b[0m\n",
      "\u001b[34m82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 404/492 [07:40<01:44,  1.19s/it]\u001b[0m\n",
      "\u001b[34m82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 405/492 [07:41<01:43,  1.18s/it]\u001b[0m\n",
      "\u001b[34m83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 406/492 [07:42<01:41,  1.18s/it]\u001b[0m\n",
      "\u001b[34m83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 407/492 [07:44<01:39,  1.18s/it]\u001b[0m\n",
      "\u001b[34m83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 408/492 [07:45<01:38,  1.18s/it]\u001b[0m\n",
      "\u001b[34m83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 409/492 [07:46<01:37,  1.18s/it]\u001b[0m\n",
      "\u001b[34m83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 410/492 [07:47<01:36,  1.18s/it]\u001b[0m\n",
      "\u001b[34m84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 411/492 [07:48<01:35,  1.18s/it]\u001b[0m\n",
      "\u001b[34m84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 412/492 [07:50<01:34,  1.18s/it]\u001b[0m\n",
      "\u001b[34m84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 413/492 [07:51<01:33,  1.18s/it]\u001b[0m\n",
      "\u001b[34m84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 414/492 [07:52<01:32,  1.18s/it]\u001b[0m\n",
      "\u001b[34m84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 415/492 [07:53<01:30,  1.18s/it]\u001b[0m\n",
      "\u001b[34m85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 416/492 [07:54<01:29,  1.18s/it]\u001b[0m\n",
      "\u001b[34m85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 417/492 [07:55<01:28,  1.18s/it]\u001b[0m\n",
      "\u001b[34m85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 418/492 [07:57<01:27,  1.18s/it]\u001b[0m\n",
      "\u001b[34m85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 419/492 [07:58<01:26,  1.18s/it]\u001b[0m\n",
      "\u001b[34m85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 420/492 [07:59<01:24,  1.18s/it]\u001b[0m\n",
      "\u001b[34m86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 421/492 [08:00<01:23,  1.18s/it]\u001b[0m\n",
      "\u001b[34m86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 422/492 [08:01<01:22,  1.18s/it]\u001b[0m\n",
      "\u001b[34m86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 423/492 [08:03<01:21,  1.19s/it]\u001b[0m\n",
      "\u001b[34m86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 424/492 [08:04<01:20,  1.19s/it]\u001b[0m\n",
      "\u001b[34m86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 425/492 [08:05<01:19,  1.19s/it]\u001b[0m\n",
      "\u001b[34m87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 426/492 [08:06<01:18,  1.19s/it]\u001b[0m\n",
      "\u001b[34m87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 427/492 [08:07<01:16,  1.18s/it]\u001b[0m\n",
      "\u001b[34m87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 428/492 [08:08<01:15,  1.18s/it]\u001b[0m\n",
      "\u001b[34m87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 429/492 [08:10<01:14,  1.19s/it]\u001b[0m\n",
      "\u001b[34m87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 430/492 [08:11<01:13,  1.19s/it]\u001b[0m\n",
      "\u001b[34m88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 431/492 [08:12<01:11,  1.18s/it]\u001b[0m\n",
      "\u001b[34m88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 432/492 [08:13<01:10,  1.18s/it]\u001b[0m\n",
      "\u001b[34m88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 433/492 [08:14<01:09,  1.18s/it]\u001b[0m\n",
      "\u001b[34m88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 434/492 [08:16<01:08,  1.18s/it]\u001b[0m\n",
      "\u001b[34m88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 435/492 [08:17<01:07,  1.19s/it]\u001b[0m\n",
      "\u001b[34m89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 436/492 [08:18<01:06,  1.18s/it]\u001b[0m\n",
      "\u001b[34m89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 437/492 [08:19<01:05,  1.18s/it]\u001b[0m\n",
      "\u001b[34m89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 438/492 [08:20<01:03,  1.18s/it]\u001b[0m\n",
      "\u001b[34m89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 439/492 [08:21<01:02,  1.18s/it]\u001b[0m\n",
      "\u001b[34m89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 440/492 [08:23<01:01,  1.18s/it]\u001b[0m\n",
      "\u001b[34m90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 441/492 [08:24<01:00,  1.18s/it]\u001b[0m\n",
      "\u001b[34m90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 442/492 [08:25<00:58,  1.18s/it]\u001b[0m\n",
      "\u001b[34m90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 443/492 [08:26<00:57,  1.18s/it]\u001b[0m\n",
      "\u001b[34m90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 444/492 [08:27<00:56,  1.18s/it]\u001b[0m\n",
      "\u001b[34m90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 445/492 [08:29<00:55,  1.19s/it]\u001b[0m\n",
      "\u001b[34m91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 446/492 [08:30<00:54,  1.18s/it]\u001b[0m\n",
      "\u001b[34m91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 447/492 [08:31<00:53,  1.19s/it]\u001b[0m\n",
      "\u001b[34m91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 448/492 [08:32<00:52,  1.18s/it]\u001b[0m\n",
      "\u001b[34m91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 449/492 [08:33<00:51,  1.19s/it]\u001b[0m\n",
      "\u001b[34m91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 450/492 [08:34<00:49,  1.19s/it]\u001b[0m\n",
      "\u001b[34m92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 451/492 [08:36<00:48,  1.19s/it]\u001b[0m\n",
      "\u001b[34m92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 452/492 [08:37<00:47,  1.19s/it]\u001b[0m\n",
      "\u001b[34m92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 453/492 [08:38<00:46,  1.19s/it]\u001b[0m\n",
      "\u001b[34m92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 454/492 [08:39<00:45,  1.19s/it]\u001b[0m\n",
      "\u001b[34m92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 455/492 [08:40<00:43,  1.18s/it]\u001b[0m\n",
      "\u001b[34m93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 456/492 [08:42<00:42,  1.19s/it]\u001b[0m\n",
      "\u001b[34m93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 457/492 [08:43<00:41,  1.19s/it]\u001b[0m\n",
      "\u001b[34m93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 458/492 [08:44<00:40,  1.19s/it]\u001b[0m\n",
      "\u001b[34m93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 459/492 [08:45<00:39,  1.19s/it]\u001b[0m\n",
      "\u001b[34m93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 460/492 [08:46<00:38,  1.19s/it]\u001b[0m\n",
      "\u001b[34m94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 461/492 [08:48<00:36,  1.19s/it]\u001b[0m\n",
      "\u001b[34m94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 462/492 [08:49<00:35,  1.19s/it]\u001b[0m\n",
      "\u001b[34m94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 463/492 [08:50<00:34,  1.18s/it]\u001b[0m\n",
      "\u001b[34m94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 464/492 [08:51<00:32,  1.18s/it]\u001b[0m\n",
      "\u001b[34m95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 465/492 [08:52<00:31,  1.18s/it]\u001b[0m\n",
      "\u001b[34m95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 466/492 [08:53<00:30,  1.19s/it]\u001b[0m\n",
      "\u001b[34m95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 467/492 [08:55<00:29,  1.19s/it]\u001b[0m\n",
      "\u001b[34m95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 468/492 [08:56<00:28,  1.18s/it]\u001b[0m\n",
      "\u001b[34m95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 469/492 [08:57<00:27,  1.18s/it]\u001b[0m\n",
      "\u001b[34m96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 470/492 [08:58<00:26,  1.18s/it]\u001b[0m\n",
      "\u001b[34m96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 471/492 [08:59<00:24,  1.18s/it]\u001b[0m\n",
      "\u001b[34m96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 472/492 [09:01<00:23,  1.18s/it]\u001b[0m\n",
      "\u001b[34m96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 473/492 [09:02<00:22,  1.18s/it]\u001b[0m\n",
      "\u001b[34m96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 474/492 [09:03<00:21,  1.18s/it]\u001b[0m\n",
      "\u001b[34m97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 475/492 [09:04<00:20,  1.18s/it]\u001b[0m\n",
      "\u001b[34m97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 476/492 [09:05<00:18,  1.18s/it]\u001b[0m\n",
      "\u001b[34m97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 477/492 [09:06<00:17,  1.18s/it]\u001b[0m\n",
      "\u001b[34m97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 478/492 [09:08<00:16,  1.18s/it]\u001b[0m\n",
      "\u001b[34m97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 479/492 [09:09<00:15,  1.18s/it]\u001b[0m\n",
      "\u001b[34m98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 480/492 [09:10<00:14,  1.18s/it]\u001b[0m\n",
      "\u001b[34m98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 481/492 [09:11<00:12,  1.18s/it]\u001b[0m\n",
      "\u001b[34m98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 482/492 [09:12<00:11,  1.18s/it]\u001b[0m\n",
      "\u001b[34m98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 483/492 [09:14<00:10,  1.18s/it]\u001b[0m\n",
      "\u001b[34m98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 484/492 [09:15<00:09,  1.18s/it]\u001b[0m\n",
      "\u001b[34m99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 485/492 [09:16<00:08,  1.18s/it]\u001b[0m\n",
      "\u001b[34m99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 486/492 [09:17<00:07,  1.18s/it]\u001b[0m\n",
      "\u001b[34m99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 487/492 [09:18<00:05,  1.18s/it]\u001b[0m\n",
      "\u001b[34m99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 488/492 [09:19<00:04,  1.18s/it]\u001b[0m\n",
      "\u001b[34m99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 489/492 [09:21<00:03,  1.18s/it]\u001b[0m\n",
      "\u001b[34m100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 490/492 [09:22<00:02,  1.18s/it]\u001b[0m\n",
      "\u001b[34m100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 491/492 [09:23<00:01,  1.19s/it]\u001b[0m\n",
      "\u001b[34m100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 492/492 [09:23<00:00,  1.11it/s]\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 737\u001b[0m\n",
      "\u001b[34mNum examples = 737\n",
      "  Batch size = 64\u001b[0m\n",
      "\u001b[34mBatch size = 64\u001b[0m\n",
      "\u001b[34m0%|          | 0/12 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m17%|â–ˆâ–‹        | 2/12 [00:02<00:10,  1.07s/it]#033[A\u001b[0m\n",
      "\u001b[34m25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:04<00:13,  1.51s/it]#033[A\u001b[0m\n",
      "\u001b[34m33%|â–ˆâ–ˆâ–ˆâ–      | 4/12 [00:06<00:13,  1.74s/it]#033[A\u001b[0m\n",
      "\u001b[34m42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:08<00:13,  1.88s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:10<00:11,  1.96s/it]#033[A\u001b[0m\n",
      "\u001b[34m58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:12<00:10,  2.02s/it]#033[A\u001b[0m\n",
      "\u001b[34m67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:14<00:08,  2.05s/it]#033[A\u001b[0m\n",
      "\u001b[34m75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:17<00:06,  2.08s/it]#033[A\u001b[0m\n",
      "\u001b[34m83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 10/12 [00:19<00:04,  2.10s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:21<00:02,  2.11s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:22<00:00,  1.80s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.30263540148735046, 'eval_accuracy': 0.903663500678426, 'eval_runtime': 24.5705, 'eval_samples_per_second': 29.995, 'eval_steps_per_second': 0.488, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 492/492 [09:48<00:00,  1.11it/s]\u001b[0m\n",
      "\u001b[34m100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:22<00:00,  1.80s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m{'train_runtime': 588.3591, 'train_samples_per_second': 10.018, 'train_steps_per_second': 0.836, 'train_loss': 0.7223663950354103, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 492/492 [09:48<00:00,  1.11it/s]#015100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 492/492 [09:48<00:00,  1.20s/it]\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 737\u001b[0m\n",
      "\u001b[34mNum examples = 737\n",
      "  Batch size = 64\u001b[0m\n",
      "\u001b[34mBatch size = 64\u001b[0m\n",
      "\u001b[34m0%|          | 0/12 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m17%|â–ˆâ–‹        | 2/12 [00:02<00:10,  1.07s/it]\u001b[0m\n",
      "\u001b[34m25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:04<00:13,  1.51s/it]\u001b[0m\n",
      "\u001b[34m33%|â–ˆâ–ˆâ–ˆâ–      | 4/12 [00:06<00:13,  1.75s/it]\u001b[0m\n",
      "\u001b[34m42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:08<00:13,  1.88s/it]\u001b[0m\n",
      "\u001b[34m50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:10<00:11,  1.97s/it]\u001b[0m\n",
      "\u001b[34m58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:12<00:10,  2.02s/it]\u001b[0m\n",
      "\u001b[34m67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:14<00:08,  2.05s/it]\u001b[0m\n",
      "\u001b[34m75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:17<00:06,  2.08s/it]\u001b[0m\n",
      "\u001b[34m83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 10/12 [00:19<00:04,  2.09s/it]\u001b[0m\n",
      "\u001b[34m92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:21<00:02,  2.10s/it]\u001b[0m\n",
      "\u001b[34m100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:22<00:00,  1.80s/it]\u001b[0m\n",
      "\u001b[34m100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:22<00:00,  1.87s/it]\u001b[0m\n",
      "\u001b[34m***** Eval results *****\u001b[0m\n",
      "\u001b[34mepoch = 1.0\u001b[0m\n",
      "\u001b[34meval_accuracy = 0.903663500678426\u001b[0m\n",
      "\u001b[34meval_loss = 0.30263540148735046\u001b[0m\n",
      "\u001b[34meval_runtime = 24.5672\u001b[0m\n",
      "\u001b[34meval_samples_per_second = 29.999\u001b[0m\n",
      "\u001b[34meval_steps_per_second = 0.488\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m2022-11-30 10:21:23,367 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2022-11-30 10:21:23,367 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2022-11-30 10:21:23,368 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-11-30 10:22:17 Uploading - Uploading generated training model\n",
      "2022-11-30 10:23:18 Completed - Training job completed\n",
      "Training seconds: 920\n",
      "Billable seconds: 920\n",
      "ã‚¸ãƒ§ãƒ–ã®åç§°: huggingface-pytorch-training-2022-11-30-10-05-45-257\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’æŒ‡å®šã—ã¦è¨“ç·´ã‚’å§‹ã‚ã‚‹\n",
    "huggingface_estimator.fit({\n",
    "    'train': train_input_path,  # è¨“ç·´ç”¨ã®ãƒ‡ãƒ¼ã‚¿\n",
    "    'test': test_input_path,    # ç²¾åº¦ç¢ºèªç”¨ã®ãƒ‡ãƒ¼ã‚¿\n",
    "})\n",
    "\n",
    "# å¾Œã‹ã‚‰å‚ç…§ã™ã‚‹ãŸã‚è¨“ç·´ã‚¸ãƒ§ãƒ–ã®åç§°ã‚’æ§ãˆã¦ãŠã\n",
    "job_name = huggingface_estimator.latest_training_job.job_name\n",
    "print(\"ã‚¸ãƒ§ãƒ–ã®åç§°:\", job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a1c962-b9a5-4f22-9406-e0aff56883fe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### è¨“ç·´ç”¨ãƒªã‚½ãƒ¼ã‚¹ã«ã¤ã„ã¦"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94412225-d79c-4aba-8ff5-35b231c334b1",
   "metadata": {},
   "source": [
    "ä¸Šã§è¨“ç·´ç”¨ã‚¸ãƒ§ãƒ–ã‚’å®šç¾©ã™ã‚‹éš›ã€`source_dir = \"s3://...\"` ã¨ã„ã†å¼•æ•°ã§è¨“ç·´ç”¨ãƒªã‚½ãƒ¼ã‚¹ã‚’æ¸¡ã—ã¦ã„ã¾ã—ãŸã€‚\n",
    "\n",
    "ã“ã® s3 arn ãŒæŒ‡ã—ã¦ã„ã‚‹ã®ã¯ã€ãƒãƒ³ã‚ºã‚ªãƒ³ã®æœ€åˆã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸ `sourcedir.tar.gz` ã¨åŒã˜ã‚‚ã®ï¼ˆã¤ã¾ã‚Šãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ä»¥å¤–ã®ãƒ•ã‚¡ã‚¤ãƒ«å…¨ã¦ï¼‰ã§ã™ã€‚\n",
    "\n",
    "è¨“ç·´ã®é–“ã«ã“ã‚Œã‚‰ã«ã¤ã„ã¦å†…å®¹ã‚’èª¬æ˜ã—ã¦ã„ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41442107-9909-427d-96b6-15bbaf42d22b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµæœã®ç¢ºèª"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bcc2ce-6a92-49c7-b3b8-5875c57e7b9d",
   "metadata": {},
   "source": [
    "è¨“ç·´ã®çµæœã€æœ€çµ‚çš„ã«ã©ã®ç¨‹åº¦ã®ç²¾åº¦ã«ãªã£ãŸã®ã‹ã‚’ç¢ºèªã—ã¾ã™ã€‚\n",
    "\n",
    "ãƒ­ã‚°ã®æœ€å¾Œã®æ–¹ã€\"***** Eval results *****\" ä»¥é™ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®ãƒ¢ãƒ‡ãƒ«è©•ä¾¡çµæœãŒå‡ºåŠ›ã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
    "`eval_accuracy = ...` ã®éƒ¨åˆ†ãŒãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®ç²¾åº¦ã§ã™ã€‚\n",
    "å­¦ç¿’ã«ã¯ä¹±æ•°ãŒé–¢ã‚ã‚‹ãŸã‚ä¸€å®šã§ã¯ã‚ã‚Šã¾ã›ã‚“ãŒã€ãŠã‚ˆã 90%ã®ç²¾åº¦ã«ãªã£ã¦ã„ã‚‹ã‹ã¨æ€ã„ã¾ã™ã€‚\n",
    "\n",
    "ä»Šå›ã®ã‚¿ã‚¹ã‚¯ã«ã¯ï¼™ã¤ã®ãƒ©ãƒ™ãƒ«ãŒã‚ã‚Šã¾ã—ãŸã€‚\n",
    "ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãªã—ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯å‡ºåŠ›ãƒ©ãƒ™ãƒ«ãŒãƒ©ãƒ³ãƒ€ãƒ ãªãŸã‚ã€ç²¾åº¦ã¯ 1/9ã€ã™ãªã‚ã¡11%ç¨‹åº¦ã¨ãªã‚Šã¾ã™ã€‚\n",
    "ã“ã‚Œã‚’è¸ã¾ãˆã‚‹ã¨ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã‚ˆã£ã¦ãƒ¢ãƒ‡ãƒ«ãŒã‚¿ã‚¹ã‚¯ã«é©å¿œã—ã€é«˜ã„ç²¾åº¦ãŒå¾—ã‚‰ã‚Œã‚‹ã‚ˆã†ã«ãªã£ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚\n",
    "\n",
    "ãªãŠä»Šå›ã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ä¸€å‘¨åˆ† (1 epoch) ã®è¨“ç·´ã‚’è¡Œã„ã¾ã—ãŸãŒã€ã“ã®é‡ã‚„ãã®ã»ã‹ã®å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´ã™ã‚‹ã“ã¨ã§æ›´ã«é«˜ã„ç²¾åº¦ãŒå¾—ã‚‰ã‚Œã‚‹å¯èƒ½æ€§ã‚‚ã‚ã‚Šã¾ã™ã€‚\n",
    "ã“ã®ã€ã‚¿ã‚¹ã‚¯ã«æœ€é©ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¢ã™ä½œæ¥­ã‚’ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ã¨å‘¼ã³ã¾ã™ãŒã€æœ¬ãƒãƒ³ã‚ºã‚ªãƒ³ã§ã¯è©³ç´°ã¯å‰²æ„›ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865cd683-6bd8-4fa3-ab49-dab813a5aedd",
   "metadata": {
    "id": "582d21fc-7e06-4e63-95a3-3d9e73673570",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## è¨“ç·´ã—ãŸãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ—ãƒ­ã‚¤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d55c44-cd28-494b-a98f-4dd22357f3fc",
   "metadata": {
    "id": "582d21fc-7e06-4e63-95a3-3d9e73673570",
    "tags": []
   },
   "source": [
    "æœ€å¾Œã«ã€å…ˆã»ã©è¨“ç·´ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã¦ã€å®Ÿéš›ã«ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ©ãƒ™ãƒ«ã‚’äºˆæ¸¬ã•ã›ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
    "\n",
    "ã“ã¡ã‚‰ã‚‚è¨“ç·´ã®æ™‚ã¨åŒæ§˜ã€SageMakerã®APIã‚’å‘¼ã¶å½¢ã«ãªã‚Šã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c8e1ec-9780-4f59-8312-2b1a583eb1ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### SageMakerã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7380276e-86cb-4255-8ade-7bedc7217152",
   "metadata": {
    "id": "7380276e-86cb-4255-8ade-7bedc7217152",
    "outputId": "36279dc7-6429-4322-8d68-b9b47b1c73be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2022-11-30 10:23:34 Starting - Preparing the instances for training\n",
      "2022-11-30 10:23:34 Downloading - Downloading input data\n",
      "2022-11-30 10:23:34 Training - Training image download completed. Training in progress.\n",
      "2022-11-30 10:23:34 Uploading - Uploading generated training model\n",
      "2022-11-30 10:23:34 Completed - Training job completed\n",
      "-----------!"
     ]
    }
   ],
   "source": [
    "# è¨“ç·´ã‚¸ãƒ§ãƒ–ã®åç§°ã§ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®š\n",
    "# ã‚‚ã—ãã¯ä¸Šã® `huggingface_estimator` ã‚’ãã®ã¾ã¾ä½¿ã†ã“ã¨ã‚‚å¯èƒ½\n",
    "estimator = HuggingFace.attach(job_name, sagemaker_session=sess)\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹\n",
    "predictor = estimator.deploy(1, \"ml.g4dn.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aad4d47-0b3f-4693-9527-74acb040f5cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### ãƒ©ãƒ™ãƒ«ã‚’äºˆæ¸¬ã•ã›ã‚‹\n",
    "ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã„ãã¤ã‹ã‚µãƒ³ãƒ—ãƒ«ã‚’ã¨ã£ã¦ã€ãƒ©ãƒ™ãƒ«ã‚’äºˆæ¸¬ã•ã›ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7485ddb1-297f-4a28-84dd-07281489dc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ©ãƒ™ãƒ«ã”ã¨ã®äºˆæ¸¬ã‚¹ã‚³ã‚¢ã‚’è¡¨ç¤ºã™ã‚‹é–¢æ•°\n",
    "def print_label_score(pred):\n",
    "    print(\"å„ãƒ©ãƒ™ãƒ«ã®ã‚¹ã‚³ã‚¢:\")\n",
    "    for l, score in sorted(enumerate(pred[\"logits\"][0]), key=lambda x: -x[1]):\n",
    "        print(f\"{id2label[l]}\\t\", f\"{score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "511cc094-8410-4b25-a3ab-46fae3023962",
   "metadata": {
    "id": "de412e3a-7693-4b51-a46a-197294ae55ee",
    "outputId": "d74139e2-f8bb-4e49-f896-8118590d3d48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "çœŸã®ãƒ©ãƒ™ãƒ«: sports-watch\n",
      "æœ¬æ–‡: é‡çƒè§£èª¬è€…ã€é‡æ‘å…‹ä¹Ÿæ°ãŒä»Šå¹´ã®é‡çƒç•Œã‚’æ¼¢å­—ä¸€æ–‡å­—ã§è¡¨ã™ã¨ï¼Ÿ TBSã®ã‚¹ãƒãƒ¼ãƒ„ç•ªçµ„ã€ŒS1ã€ï¼ˆ25æ—¥æ·±å¤œæ”¾é€ï¼‰ã‚‚å¹´å†…æœ€å¾Œã®æ”¾é€ã¨ãªã‚Šã€äººæ°—ã‚³ãƒ¼ãƒŠãƒ¼ã€Œãƒãƒ ã•ã‚“ã¼ã‚„ãéƒ¨å±‹ã€ã‚‚ã“ã®æ—¥ã®æ”¾é€ã§ãƒœãƒ¤ã‚­ãŠã•ã‚ã¨ãªã£ãŸã€‚ãã‚“ãªä»Šå¹´æœ€å¾Œã®ãƒ†ãƒ¼ãƒã¯å‰è¿°ã—ãŸé€šã‚Šâ€”â€”ã€‚ ç™»å ´ã™ã‚‹ã‚„ã€TBSæ¡ç”°çµµç†å¥ˆã‚¢ãƒŠã‚¦ãƒ³ã‚µãƒ¼ã«ã€Œèª•ç”Ÿæ—¥ãŠã‚ã§ã¨ã†ã€ã¨èŠ±æŸã‚’æ¸¡ã—ãŸãƒãƒ ã•ã‚“ã€‚12æœˆ25æ—¥ã§26æ­³ã«ãªã£ãŸæ¡ç”°ã‚¢ãƒŠã«ã€Œ36æ­³ã‹ï¼Ÿã€ã¨ã„ã†ãƒœã‚±ã‚’ã‹ã¾...\n",
      "\n",
      "äºˆæ¸¬ãƒ©ãƒ™ãƒ«: sports-watch\n",
      "å„ãƒ©ãƒ™ãƒ«ã®ã‚¹ã‚³ã‚¢:\n",
      "sports-watch\t 4.21\n",
      "topic-news\t 4.06\n",
      "it-life-hack\t 0.21\n",
      "livedoor-homme\t -0.58\n",
      "movie-enter\t -0.64\n",
      "kaden-channel\t -0.93\n",
      "smax\t -1.23\n",
      "peachy\t -1.63\n",
      "dokujo-tsushin\t -2.08\n"
     ]
    }
   ],
   "source": [
    "# ãƒ©ãƒ³ãƒ€ãƒ ãªãƒ‡ãƒ¼ã‚¿ã‚’é¸æŠ\n",
    "d = livedoor_data.shuffle()[0]\n",
    "text = d[\"sentence1\"]\n",
    "label = d[\"label\"]\n",
    "\n",
    "print(\"çœŸã®ãƒ©ãƒ™ãƒ«:\", label)\n",
    "print(f\"æœ¬æ–‡: {text[:200]}...\\n\")\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã«ãƒ©ãƒ™ãƒ«ã‚’äºˆæ¸¬ã•ã›ã‚‹\n",
    "pred = predictor.predict({\"inputs\": text})\n",
    "\n",
    "print(\"äºˆæ¸¬ãƒ©ãƒ™ãƒ«:\", id2label[pred[\"label\"]])\n",
    "print_label_score(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ddfbb56-9d20-4134-b345-e5af29fe9ea8",
   "metadata": {
    "id": "de412e3a-7693-4b51-a46a-197294ae55ee",
    "outputId": "d74139e2-f8bb-4e49-f896-8118590d3d48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "äºˆæ¸¬ãƒ©ãƒ™ãƒ«: dokujo-tsushin\n",
      "å„ãƒ©ãƒ™ãƒ«ã®ã‚¹ã‚³ã‚¢:\n",
      "dokujo-tsushin\t 1.83\n",
      "kaden-channel\t 0.76\n",
      "livedoor-homme\t 0.59\n",
      "it-life-hack\t 0.20\n",
      "sports-watch\t -0.24\n",
      "movie-enter\t -0.29\n",
      "smax\t -0.48\n",
      "topic-news\t -1.04\n",
      "peachy\t -1.12\n"
     ]
    }
   ],
   "source": [
    "# æ–°è¦ã®æ–‡ç« ã‚’å…¥åŠ›ã¨ã™ã‚‹\n",
    "text = \"å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ã€‚\"\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã«ãƒ©ãƒ™ãƒ«ã‚’äºˆæ¸¬ã•ã›ã‚‹\n",
    "pred = predictor.predict({\"inputs\": text})\n",
    "\n",
    "print(\"äºˆæ¸¬ãƒ©ãƒ™ãƒ«:\", id2label[pred[\"label\"]])\n",
    "print_label_score(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad7acee-a760-481b-bd0f-a599b01dbe3a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### å¾Œå‡¦ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d51bdfc-b160-4277-bbcd-db663e1b2e73",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "æœ€å¾Œã«ãƒ‡ãƒ—ãƒ­ã‚¤ã—ãŸã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å‰Šé™¤ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d99bf1c8-c44e-442a-95b0-96668fc695fa",
   "metadata": {
    "id": "d99bf1c8-c44e-442a-95b0-96668fc695fa"
   },
   "outputs": [],
   "source": [
    "# é–“é•ãˆã¦å‰Šé™¤ã—ã¦ã—ã¾ã‚ãªã„ã‚ˆã†ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã¦ã„ã‚‹\n",
    "# è¡Œé ­ã® \"#\" ã‚’å‰Šé™¤ã—ã¦ã‹ã‚‰å®Ÿè¡Œã™ã‚‹\n",
    "\n",
    "# predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
