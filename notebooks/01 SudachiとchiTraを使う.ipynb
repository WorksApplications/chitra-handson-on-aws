{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c73f6f57-5b12-4e5d-a935-c1afe1cf71ff",
   "metadata": {
    "id": "f2891a57-f323-4c1a-a702-27285f54f60f",
    "tags": []
   },
   "source": [
    "# SudachiとchiTraを使う"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f4c50e-72e1-44c2-a591-8e102547cb51",
   "metadata": {
    "id": "f2891a57-f323-4c1a-a702-27285f54f60f"
   },
   "source": [
    "このノートブックでは、\n",
    "- 日本語形態素解析器Sudachi\n",
    "- 事前学習済み言語モデル+トークナイザchiTra\n",
    "\n",
    "の二つを実際に利用し、何が行えるのかを理解することがゴールです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u6kwqQCJ9lHU",
   "metadata": {
    "id": "u6kwqQCJ9lHU",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22460fb3-4432-42f6-89a7-ee2c0e9d5513",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 出力の消去\n",
    "配布のノートブックでは、各セルの実行結果を参照用に残しています。\n",
    "\n",
    "作業においては実行場所がわかりにくくなるので、右クリックのメニューから`Clear All Outputs`を実行して消去します。\n",
    "\n",
    "本来どのようになるのか確認したい、初期状態に戻したいなどの場合は、\n",
    "ターミナルからコマンド `tar -xvf notebooks.tar.gz` で再度展開を行ってください（ファイルの上書きにご注意ください）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86288f66-93dd-4fb3-a356-98390022b098",
   "metadata": {
    "tags": []
   },
   "source": [
    "### chiTra モデルのダウンロード\n",
    "chiTraのモデルデータは[公式github](https://github.com/WorksApplications/SudachiTra)にて配布されています。\n",
    "通常はそちらからダウンロードして利用することになります。\n",
    "\n",
    "今回はこのノートブックとともにs3からダウンロードしているので、ここではパスのみ設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f859d1e-3d15-4164-821e-b7b8e88a6c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "chitra_path = \"./chiTra-1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a88cbae-4df8-4955-9490-bab153dc0b51",
   "metadata": {
    "id": "0Pc-KMlD9kTu",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Sudachi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4020a1b6-ed89-49de-94f5-da5b5ecd58cb",
   "metadata": {
    "id": "0Pc-KMlD9kTu",
    "tags": []
   },
   "source": [
    "### Sudachi とは\n",
    "Sudachi は日本語形態素解析器です。\n",
    "\n",
    "形態素解析とはテキストを形態素（~= 単語）単位に分割する操作です。\n",
    "\n",
    "これによって語の区切りを計算機で扱えるようにするほか、辞書データから品詞や正規形といった追加情報も取得できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bff6ec-b984-48d4-a6f8-08eef6b6c960",
   "metadata": {
    "id": "0Pc-KMlD9kTu",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 基本的な使い方"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fcc059-941d-4a53-ad22-cc294aaf0166",
   "metadata": {},
   "source": [
    "Sudachi の基本的な使い方は、\n",
    "- トークナイザを構成する\n",
    "- 処理したいテキストを渡す\n",
    "\n",
    "の二段階です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4735687-642a-412a-acac-022f0ac77457",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1668739992107,
     "user": {
      "displayName": "林政義",
      "userId": "06671713239305786665"
     },
     "user_tz": -540
    },
    "id": "Zm81ZtNjFLjH"
   },
   "outputs": [],
   "source": [
    "import sudachipy\n",
    "\n",
    "# 辞書を読み込む\n",
    "dictionary = sudachipy.Dictionary()\n",
    "\n",
    "# トークナイザを生成\n",
    "tokenizer = dictionary.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcac6dac-b27c-46d5-afc8-574e77d9734b",
   "metadata": {},
   "source": [
    "好きな文章を解析させてみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "jiQ5x3HgFSfL",
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "aborted",
     "timestamp": 1668739776197,
     "user": {
      "displayName": "林政義",
      "userId": "06671713239305786665"
     },
     "user_tz": -540
    },
    "id": "jiQ5x3HgFSfL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "吾輩 は 猫 で ある 。\n"
     ]
    }
   ],
   "source": [
    "# 処理対象となる文章を設定\n",
    "text = \"吾輩は猫である。\"\n",
    "\n",
    "# 解析を実行\n",
    "morphemes = tokenizer.tokenize(text)\n",
    "\n",
    "# 空白区切りで表示\n",
    "print(morphemes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330161d3-67ee-4451-883c-8a97b3a19bce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 語の情報を取得する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286e0c81-fa76-4799-a7d9-65c190681ec1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "分割を行うのみでなく、辞書から語の情報を取得することもできます。\n",
    "どのような情報が得られるのか確認してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9gCfAkFGFSc4",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "aborted",
     "timestamp": 1668739776198,
     "user": {
      "displayName": "林政義",
      "userId": "06671713239305786665"
     },
     "user_tz": -540
    },
    "id": "9gCfAkFGFSc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "表層形：吾輩\n",
      "辞書形：吾輩\n",
      "正規形：我が輩\n",
      "読み：ワガハイ\n",
      "品詞：('代名詞', '*', '*', '*', '*', '*')\n"
     ]
    }
   ],
   "source": [
    "text = \"吾輩は猫である。\"\n",
    "morphemes = tokenizer.tokenize(text)\n",
    "\n",
    "# 最初の形態素について表示\n",
    "m = morphemes[0]\n",
    "\n",
    "print(f\"表層形：{m.surface()}\")\n",
    "print(f\"辞書形：{m.dictionary_form()}\")\n",
    "print(f\"正規形：{m.normalized_form()}\")\n",
    "print(f\"読み：{m.reading_form()}\")\n",
    "print(f\"品詞：{m.part_of_speech()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44734f2b-fdeb-43b6-872b-311b8d4f63e2",
   "metadata": {},
   "source": [
    "これらの情報を使うと、例えば以下のようなことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dea82fc-841b-4b97-88b5-174e0b4711b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ワガハイハネコデアル。ナマエハマダナイ。\n",
      "['猫', '名前']\n"
     ]
    }
   ],
   "source": [
    "text = \"吾輩は猫である。名前はまだない。\"\n",
    "\n",
    "morphemes = tokenizer.tokenize(text)\n",
    "\n",
    "# 読みに変換\n",
    "print(\"\".join(m.reading_form() for m in morphemes))\n",
    "\n",
    "# 名詞のみを取り出す\n",
    "print([m.surface() for m in morphemes if \"名詞\" == m.part_of_speech()[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8948e687-2ded-4d30-b640-1e80d62763ab",
   "metadata": {
    "id": "m7MdomcmpHDz",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 分割単位の変更"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ff54a8-dc7a-40f9-9a07-d52419eb8aaa",
   "metadata": {
    "id": "m7MdomcmpHDz"
   },
   "source": [
    "Sudachi には複数の分割単位が用意されています。\n",
    "これによりタスクでの必要に応じて、より細かい/大きな単位での分割を行うことができます。\n",
    "\n",
    "どのような差があるか見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "918y7GB6pgsE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 386,
     "status": "ok",
     "timestamp": 1668740033563,
     "user": {
      "displayName": "林政義",
      "userId": "06671713239305786665"
     },
     "user_tz": -540
    },
    "id": "918y7GB6pgsE",
    "outputId": "c41ded0b-02e7-4f82-c8fb-bf443add3354"
   },
   "outputs": [],
   "source": [
    "# 各分割単位のトークナイザを用意\n",
    "tokA = dictionary.create(mode=sudachipy.SplitMode.A)\n",
    "tokB = dictionary.create(mode=sudachipy.SplitMode.B)\n",
    "tokC = dictionary.create(mode=sudachipy.SplitMode.C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1Unr97l5pgP4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 500,
     "status": "ok",
     "timestamp": 1668740088752,
     "user": {
      "displayName": "林政義",
      "userId": "06671713239305786665"
     },
     "user_tz": -540
    },
    "id": "1Unr97l5pgP4",
    "outputId": "2dd24577-d041-4b6d-85df-6fe2d014cb76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A単位: 外国 人 参政 権\n",
      "B単位: 外国人 参政権\n",
      "C単位: 外国人参政権\n"
     ]
    }
   ],
   "source": [
    "text = \"外国人参政権\"\n",
    "\n",
    "# 各分割単位で解析を実行する\n",
    "print(\"A単位:\", tokA.tokenize(text))\n",
    "print(\"B単位:\", tokB.tokenize(text))\n",
    "print(\"C単位:\", tokC.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g8Qtxd3xnJvb",
   "metadata": {
    "id": "g8Qtxd3xnJvb",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## chiTra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2dbe55-538c-4292-8c08-f7fe5c696f59",
   "metadata": {
    "id": "9sU-RIOLolfw",
    "tags": []
   },
   "source": [
    "### chiTra とは\n",
    "\n",
    "chiTra は事前学習済み言語モデルと、それを利用するためのトークナイザを提供します。\n",
    "\n",
    "#### 事前学習済み言語モデルとは\n",
    "自然文はそのままでは計算機で扱うのに適していないため、何らかの方法で数値ベクトルに変換するのが一般的です。\n",
    "例えば形態素解析で分割し、各単語の出現数をカウントするといった方法があります。\n",
    "\n",
    "近年はこの変換からタスクへの応用までをニューラルモデルで一括して行う手法が注目されています。\n",
    "モデルをゼロから学習するのは大変なので、\"汎用的に良い\"変換を大規模なテキストデータから\n",
    "事前に学習させておいたものをベースとして使うのが一般的で、chiTraモデルもその一つです。\n",
    "\n",
    "さらに詳しくは、[過去のテックトーク](https://worksapplications.connpass.com/event/244032/)の資料もご参照ください。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8186888-5f6f-42e2-80c9-49d54f8f3655",
   "metadata": {
    "id": "o9nFMbQmuECX",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### chiTra トークナイザ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c808c084-598b-4059-ab3e-3cb7395e9783",
   "metadata": {
    "id": "rppE_zqmvEkJ"
   },
   "source": [
    "chiTra モデルはテキストをそのまま扱うわけではなく、\n",
    "モデル学習時に登録しているトークンの列を入力として受け取ります。\n",
    "\n",
    "テキストをこの列に変換する役割を持つのが chiTra トークナイザです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69960a1-5fe9-4e25-934a-7d8158efa972",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### テキストの変換"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7956d94b-c56e-4e0c-a12c-5516fb282cf4",
   "metadata": {
    "id": "rppE_zqmvEkJ"
   },
   "source": [
    "テキストがどのようなトークン列に変換されるかを見てみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8efb41a2-616f-4cf3-9c13-9383a5d14973",
   "metadata": {
    "id": "7AMCfkfWFSaH"
   },
   "outputs": [],
   "source": [
    "import sudachitra as chitra\n",
    "\n",
    "# モデルデータからトークナイザを読み込む\n",
    "tokenizer = chitra.BertSudachipyTokenizer.from_pretrained(chitra_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44a24894-f634-4fde-a2a9-8d1e607293b9",
   "metadata": {
    "id": "35dbe64f-ee88-4384-9105-0a65d587c146",
    "outputId": "240a66f4-9298-4e15-a870-11b840088b15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我が', '##輩', 'は', '猫', 'で', 'ある', '。', '名前', 'は', '未だ', 'ない', '。']\n"
     ]
    }
   ],
   "source": [
    "text = \"吾輩は猫である。名前はまだない。\"\n",
    "\n",
    "# 文章をトークンの列に分割する\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7f4304-c385-407a-8f3f-3ed1fc6f2909",
   "metadata": {
    "id": "5e7f4304-c385-407a-8f3f-3ed1fc6f2909",
    "tags": []
   },
   "source": [
    "実際には語の列ではなく語の番号の列（と補助情報）が渡されることになります。\n",
    "トークナイザを関数として使用すると、モデルに実際に渡される形式での出力を確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48159ec2-830c-4cde-beeb-183ade6c2bd9",
   "metadata": {
    "id": "48159ec2-830c-4cde-beeb-183ade6c2bd9",
    "outputId": "2f73d43e-8ff6-4d3a-f6c1-a8fe9a9fdbf1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 14108, 6619, 485, 2851, 477, 10149, 419, 10793, 485, 11207, 10160, 419, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"吾輩は猫である。名前はまだない。\"\n",
    "\n",
    "tokenizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2bf752-6fb8-44f7-9d76-0bbabf1f9b5d",
   "metadata": {
    "id": "4a4177f9-2f83-486d-adbc-6fb5965ba715",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### word_form_type について"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540b8a0a-69be-4673-894d-12a3e9b59c8c",
   "metadata": {
    "id": "4a4177f9-2f83-486d-adbc-6fb5965ba715",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "分割後のトークンの一部が、元のテキストとは違う表記に変換されていることにお気づきでしょうか。\n",
    "\n",
    "chiTraトークナイザは、辞書情報に基づいて語の正規化を行う機能を持っています。\n",
    "今使用しているchiTra-1.0は `normalized_and_surface` モードが採用されており、\n",
    "活用しない語（名詞など）は表記にかかわらず共通の形で出力されるようになっています。\n",
    "\n",
    "ここでは他のモードではどのような出力になるかを見てみましょう。\n",
    "\n",
    "なお以下では簡単のため chiTra-1.0 のデータを使用しますが、本来は設定ごとに構築する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a860ced-ecb0-46b7-b2be-4343fea56e7e",
   "metadata": {
    "id": "88438cbe-4c4a-48af-9a37-9adff26a0ced",
    "outputId": "3b6b2fef-5c6c-4190-cbcf-189b8d73ed04"
   },
   "outputs": [],
   "source": [
    "# surface: 正規化なし\n",
    "tok1 = chitra.BertSudachipyTokenizer.from_pretrained(chitra_path, word_form_type=\"surface\")\n",
    "\n",
    "# normalized_and_surface: 活用しない語のみ正規化\n",
    "tok2 = chitra.BertSudachipyTokenizer.from_pretrained(chitra_path, word_form_type=\"normalized_and_surface\")\n",
    "\n",
    "# normalized: 全て正規化\n",
    "tok3 = chitra.BertSudachipyTokenizer.from_pretrained(chitra_path, word_form_type=\"normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd8490bd-48a0-4c1b-9023-9041a676d583",
   "metadata": {
    "id": "88438cbe-4c4a-48af-9a37-9adff26a0ced",
    "outputId": "3b6b2fef-5c6c-4190-cbcf-189b8d73ed04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['引', '##越', 'し', 'て', 'から', 'す', '##だ', '##ち', 'を', '届け', 'ます']\n",
      "['引っ越し', 'し', 'て', 'から', '酢', '##橘', 'を', '届け', 'ます']\n",
      "['引っ越し', '為', '##る', 'て', 'から', '酢', '##橘', 'を', '届け', '##る', 'ます']\n"
     ]
    }
   ],
   "source": [
    "text = \"引越してからすだちを届けます\"\n",
    "\n",
    "print(tok1.tokenize(text))\n",
    "print(tok2.tokenize(text))\n",
    "print(tok3.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02977ef4-b75d-4ac2-9fee-eeb1e9a8b8fe",
   "metadata": {
    "id": "02977ef4-b75d-4ac2-9fee-eeb1e9a8b8fe"
   },
   "source": [
    "この正規化により、以下のような利点が期待されます。\n",
    "- 入力文での語の表記ゆれをトークナイザの段階で統一し、モデルでは同じ語彙として扱える。\n",
    "- これに伴って統一される分だけ語彙を多く扱うことができる。\n",
    "\n",
    "逆に以下のようなことも想定されるので、タスクに合わせて選択するとよいでしょう。\n",
    "- 表記の違いによるニュアンスが失われる（例：人、ひと、ヒト）。\n",
    "- モードによっては語の活用などの情報が失われる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b621676-24fb-4059-be2f-2f14adb49460",
   "metadata": {
    "id": "59b48ab2-a697-4173-8e8d-ba10026e5d0b",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### chiTra モデル"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d008db-89f1-4569-a76c-21d4a76eaba6",
   "metadata": {
    "id": "59b48ab2-a697-4173-8e8d-ba10026e5d0b"
   },
   "source": [
    "次に事前学習済み言語モデルchiTraを使ってみます。\n",
    "\n",
    "chiTraモデルは HuggingFaceの Transformersフレームワークを採用しています。\n",
    "今回はこのフレームワークの機能を経由してモデルを利用します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d1d2d2-72a7-43ba-943e-60225019080a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 生の出力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eaf9cb-dea1-4db1-b5a3-bf0da6b5f289",
   "metadata": {
    "id": "59b48ab2-a697-4173-8e8d-ba10026e5d0b"
   },
   "source": [
    "前節でchiTraモデルによって入力テキストをベクトルに変換できるという説明をしました。\n",
    "まずはどのようなベクトルになるのか、生の出力を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "495c6130-b221-4f5d-ad60-3e94b6017360",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chiTra-1.0 were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "model = transformers.BertModel.from_pretrained(chitra_path)\n",
    "tok = chitra.BertSudachipyTokenizer.from_pretrained(chitra_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f015547d-8ea4-4b7b-9fc9-1b2c592735fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "トークンの数: 14\n",
      "最終層のサイズ: torch.Size([1, 14, 768])\n",
      "最終層の出力: tensor([[[ 1.1296, -1.3310, -1.2289,  ..., -2.3355, -1.0474, -0.3329],\n",
      "         [ 0.2558, -0.5519, -0.0475,  ...,  0.9909,  0.8790,  2.5155],\n",
      "         [-0.9614, -1.9350, -0.5023,  ..., -0.0623, -1.0334,  2.0975],\n",
      "         ...,\n",
      "         [ 1.1879, -2.0290, -1.3328,  ..., -0.8317, -1.8916,  1.0683],\n",
      "         [ 1.1005, -1.6217,  0.0045,  ...,  0.3315, -0.4536,  2.7701],\n",
      "         [ 1.2624, -2.4691, -1.7452,  ..., -0.7947, -0.1858,  1.8848]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "text = \"吾輩は猫である。名前はまだない。\"\n",
    "\n",
    "inputs = tok(text, return_tensors=\"pt\")\n",
    "print(\"トークンの数:\", inputs[\"input_ids\"].shape[1])\n",
    "\n",
    "result = model(**inputs)\n",
    "print(\"最終層のサイズ:\", result.last_hidden_state.shape)\n",
    "print(\"最終層の出力:\", result.last_hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7772f258-935d-4fce-b687-f600fbd70a4e",
   "metadata": {},
   "source": [
    "モデル最終層の出力のサイズから、各トークンが 768次元のベクトルに変換されていることがわかります。\n",
    "これがさらに別の層に渡され、個別のタスクを学習・実行するもととなります。\n",
    "\n",
    "とはいえベクトルのままでは人目には理解できないので、次に実際の応用を見てみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2169d76-1287-49fa-bc72-1ab095bd511d",
   "metadata": {
    "id": "59b48ab2-a697-4173-8e8d-ba10026e5d0b",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### マスクされた語を予測する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HqS60yHdyaGf",
   "metadata": {
    "id": "HqS60yHdyaGf"
   },
   "source": [
    "chiTraモデルはBERTというニューラルモデルのアーキテクチャを採用しています。\n",
    "これは「二つの文が連続したものかを判定する」「文中のマスクされた語を予測する」の二つのタスクを用いて学習されるモデルです。\n",
    "\n",
    "したがってこれらのタスクについては追加の学習なしでもタスクを動作させることができます。\n",
    "ここではこのうち「マスクされた語の予測」を実際に実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "663008a2-04b3-422f-a450-1520e09566af",
   "metadata": {
    "id": "663008a2-04b3-422f-a450-1520e09566af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chiTra-1.0 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForMaskedLM.from_pretrained(chitra_path)\n",
    "\n",
    "fillmask = transformers.FillMaskPipeline(model, tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02fd8d90-a48c-4275-be46-5542eac3d330",
   "metadata": {
    "id": "02fd8d90-a48c-4275-be46-5542eac3d330",
    "outputId": "0079f46a-1c60-4766-e4ad-649ca4cb285c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.0613539032638073,\n",
       "  'token': 17527,\n",
       "  'token_str': 'ペ ッ ト',\n",
       "  'sequence': '我が輩 は ペット で 或る'},\n",
       " {'score': 0.051257725805044174,\n",
       "  'token': 2851,\n",
       "  'token_str': '猫',\n",
       "  'sequence': '我が輩 は 猫 で 或る'},\n",
       " {'score': 0.011168411001563072,\n",
       "  'token': 27947,\n",
       "  'token_str': 'プ ロ グ ラ マ ー',\n",
       "  'sequence': '我が輩 は プログラマー で 或る'},\n",
       " {'score': 0.009473491460084915,\n",
       "  'token': 10732,\n",
       "  'token_str': '人 間',\n",
       "  'sequence': '我が輩 は 人間 で 或る'},\n",
       " {'score': 0.008770175278186798,\n",
       "  'token': 26303,\n",
       "  'token_str': 'サ ラ リ ー マ ン',\n",
       "  'sequence': '我が輩 は サラリーマン で 或る'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 予測したい部分を「[MASK]」に置き換えたテキスト\n",
    "text = \"吾輩は[MASK]である\"\n",
    "\n",
    "fillmask(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94960c6c-287d-4802-96ac-8b5a317ad138",
   "metadata": {
    "id": "iahz4h-Oy9jV",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 別のタスクへの応用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a414717d-b98b-4eb5-9e84-ff46586f4315",
   "metadata": {
    "id": "iahz4h-Oy9jV"
   },
   "source": [
    "chiTraモデルが採用するBERTアーキテクチャは、各種タスクへの応用力の高さで知られています。\n",
    "モデルの出力部分を目的のタスクに合わせて組み替えることで、上で見たような言語の理解を様々なタスクに応用できるものとされています。\n",
    "\n",
    "しかし、入れ替えのみで即座に高い性能が得られるわけではなく、対象とするタスクのデータを用いて追加の調整が必要になります。\n",
    "これが次のステップで見るファインチューニングというものです。\n",
    "\n",
    "このノートブックの最後に、chiTraモデルを文章を分類するタスクのための形式で読み込んで、\n",
    "そのままでは使えないことを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcc5976d-6edd-413d-a6b5-ece1a96c8068",
   "metadata": {
    "id": "bcc5976d-6edd-413d-a6b5-ece1a96c8068"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chiTra-1.0 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./chiTra-1.0 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 文のカテゴリを予測する形式でモデルを読み込む\n",
    "model = transformers.BertForSequenceClassification.from_pretrained(chitra_path)\n",
    "textclsf = transformers.TextClassificationPipeline(model=model, tokenizer=tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cab0ffaa-a1e0-4962-853c-0f3a40c059f4",
   "metadata": {
    "id": "cab0ffaa-a1e0-4962-853c-0f3a40c059f4",
    "outputId": "5428b4dc-dd1c-4048-b7c7-3aaa416952da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.5666131973266602}]\n",
      "[{'label': 'LABEL_0', 'score': 0.5520848631858826}]\n"
     ]
    }
   ],
   "source": [
    "# ラベルとスコアが出力されるが、期待するような規則性はない\n",
    "text = \"吾輩は猫である[SEP]名前はまだない\"\n",
    "print(textclsf(text))\n",
    "\n",
    "text = \"引っ越してからすだちを送ります\"\n",
    "print(textclsf(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d09d02f-ceaf-4953-a1e8-2482f1bf0c7b",
   "metadata": {},
   "source": [
    "繰り返しになりますが、この時点ではモデルは入力のトークン列をベクトルに変換するまではできるものの、\n",
    "それをもとにどのように分類を行うか、の基準を持っていません。\n",
    "その部分を担うことになるモデル最終層はこの時点ではランダムに初期化されており、したがって出力のラベルもランダムなものになります。\n",
    "\n",
    "次のノートブックでは、この最終層を学習するファインチューニングを行っていきます。"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
